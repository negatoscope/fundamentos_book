

<!DOCTYPE html>


<html lang="en" >

  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="generator" content="Docutils 0.18.1: http://docutils.sourceforge.net/" />

    <title>Introducción a la probabilidad &#8212; Apuntes Fundamentos de Investigación II</title>
  
  
  
  <script data-cfasync="false">
    document.documentElement.dataset.mode = localStorage.getItem("mode") || "";
    document.documentElement.dataset.theme = localStorage.getItem("theme") || "light";
  </script>
  
  <!-- Loaded before other Sphinx assets -->
  <link href="../_static/styles/theme.css?digest=e353d410970836974a52" rel="stylesheet" />
<link href="../_static/styles/bootstrap.css?digest=e353d410970836974a52" rel="stylesheet" />
<link href="../_static/styles/pydata-sphinx-theme.css?digest=e353d410970836974a52" rel="stylesheet" />

  
  <link href="../_static/vendor/fontawesome/6.1.2/css/all.min.css?digest=e353d410970836974a52" rel="stylesheet" />
  <link rel="preload" as="font" type="font/woff2" crossorigin href="../_static/vendor/fontawesome/6.1.2/webfonts/fa-solid-900.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../_static/vendor/fontawesome/6.1.2/webfonts/fa-brands-400.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../_static/vendor/fontawesome/6.1.2/webfonts/fa-regular-400.woff2" />

    <link rel="stylesheet" type="text/css" href="../_static/pygments.css" />
    <link rel="stylesheet" href="../_static/styles/sphinx-book-theme.css?digest=14f4ca6b54d191a8c7657f6c759bf11a5fb86285" type="text/css" />
    <link rel="stylesheet" type="text/css" href="../_static/togglebutton.css" />
    <link rel="stylesheet" type="text/css" href="../_static/copybutton.css" />
    <link rel="stylesheet" type="text/css" href="../_static/mystnb.4510f1fc1dee50b3e5859aac5469c37c29e427902b24a333a5f9fcb2f0b3ac41.css" />
    <link rel="stylesheet" type="text/css" href="../_static/sphinx-thebe.css" />
    <link rel="stylesheet" type="text/css" href="../_static/design-style.4045f2051d55cab465a707391d5b2007.min.css" />
  
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="../_static/scripts/bootstrap.js?digest=e353d410970836974a52" />
<link rel="preload" as="script" href="../_static/scripts/pydata-sphinx-theme.js?digest=e353d410970836974a52" />

    <script data-url_root="../" id="documentation_options" src="../_static/documentation_options.js"></script>
    <script src="../_static/jquery.js"></script>
    <script src="../_static/underscore.js"></script>
    <script src="../_static/_sphinx_javascript_frameworks_compat.js"></script>
    <script src="../_static/doctools.js"></script>
    <script src="../_static/clipboard.min.js"></script>
    <script src="../_static/copybutton.js"></script>
    <script src="../_static/scripts/sphinx-book-theme.js?digest=5a5c038af52cf7bc1a1ec88eea08e6366ee68824"></script>
    <script>let toggleHintShow = 'Click to show';</script>
    <script>let toggleHintHide = 'Click to hide';</script>
    <script>let toggleOpenOnPrint = 'true';</script>
    <script src="../_static/togglebutton.js"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script src="../_static/design-tabs.js"></script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"
const thebe_selector = ".thebe,.cell"
const thebe_selector_input = "pre"
const thebe_selector_output = ".output, .cell_output"
</script>
    <script async="async" src="../_static/sphinx-thebe.js"></script>
    <script>window.MathJax = {"options": {"processHtmlClass": "tex2jax_process|mathjax_process|math|output_area"}}</script>
    <script defer="defer" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script>DOCUMENTATION_OPTIONS.pagename = 'docs/probability_jupyter';</script>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <meta name="docsearch:language" content="en"/>
  </head>
  
  
  <body data-bs-spy="scroll" data-bs-target=".bd-toc-nav" data-offset="180" data-bs-root-margin="0px 0px -60%" data-default-mode="">

  
  
  <a class="skip-link" href="#main-content">Skip to main content</a>
  
  <input type="checkbox"
          class="sidebar-toggle"
          name="__primary"
          id="__primary"/>
  <label class="overlay overlay-primary" for="__primary"></label>
  
  <input type="checkbox"
          class="sidebar-toggle"
          name="__secondary"
          id="__secondary"/>
  <label class="overlay overlay-secondary" for="__secondary"></label>
  
  <div class="search-button__wrapper">
    <div class="search-button__overlay"></div>
    <div class="search-button__search-container">
<form class="bd-search d-flex align-items-center"
      action="../search.html"
      method="get">
  <i class="fa-solid fa-magnifying-glass"></i>
  <input type="search"
         class="form-control"
         name="q"
         id="search-input"
         placeholder="Search this book..."
         aria-label="Search this book..."
         autocomplete="off"
         autocorrect="off"
         autocapitalize="off"
         spellcheck="false"/>
  <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form></div>
  </div>
  
    <nav class="bd-header navbar navbar-expand-lg bd-navbar">
    </nav>
  
  <div class="bd-container">
    <div class="bd-container__inner bd-page-width">
      
      <div class="bd-sidebar-primary bd-sidebar">
        

  
  <div class="sidebar-header-items sidebar-primary__section">
    
    
    
    
  </div>
  
    <div class="sidebar-primary-items__start sidebar-primary__section">
        <div class="sidebar-primary-item">
  

<a class="navbar-brand logo" href="index.html">
  
  
  
  
    
    
      
    
    
    <img src="../_static/qe-logo-large.png" class="logo__image only-light" alt="Logo image"/>
    <script>document.write(`<img src="../_static/qe-logo-large.png" class="logo__image only-dark" alt="Logo image"/>`);</script>
  
  
</a></div>
        <div class="sidebar-primary-item"><nav class="bd-links" id="bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item navbar-nav active">
        <ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="probability.html">1 Probabilidad y Distribuciones</a></li>
<li class="toctree-l1"><a class="reference internal" href="estimation.html">2 Estimación</a></li>
<li class="toctree-l1"><a class="reference internal" href="hypothesis.html">3 Contraste de hipótesis</a></li>
</ul>

    </div>
</nav></div>
    </div>
  
  
  <div class="sidebar-primary-items__end sidebar-primary__section">
  </div>
  
  <div id="rtd-footer-container"></div>


      </div>
      
      <main id="main-content" class="bd-main">
        
        

<div class="sbt-scroll-pixel-helper"></div>

          <div class="bd-content">
            <div class="bd-article-container">
              
              <div class="bd-header-article">
<div class="header-article-items header-article__inner">
  
    <div class="header-article-items__start">
      
        <div class="header-article-item"><label class="sidebar-toggle primary-toggle btn btn-sm" for="__primary" title="Toggle primary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
  <span class="fa-solid fa-bars"></span>
</label></div>
      
    </div>
  
  
    <div class="header-article-items__end">
      
        <div class="header-article-item">

<div class="article-header-buttons">





<div class="dropdown dropdown-launch-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Launch interactive content">
    <i class="fas fa-rocket"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="https://mybinder.org/v2/gh/negatoscope/fundamentos_book/main?urlpath=tree/mini_book/docs/probability_jupyter.ipynb" target="_blank"
   class="btn btn-sm dropdown-item"
   title="Launch onBinder"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  
    <img src="../_static/images/logo_binder.svg">
  </span>
<span class="btn__text-container">Binder</span>
</a>
</li>
      
      
      
      
      <li>
<button onclick="initThebeSBT()"
  class="btn btn-sm btn-launch-thebe dropdown-item"
  title="Launch Thebe"
  data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-play"></i>
  </span>
<span class="btn__text-container">Live Code</span>
</button>
</li>
      
  </ul>
</div>






<div class="dropdown dropdown-download-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Download this page">
    <i class="fas fa-download"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="../_sources/docs/probability_jupyter.ipynb" target="_blank"
   class="btn btn-sm btn-download-source-button dropdown-item"
   title="Download source file"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file"></i>
  </span>
<span class="btn__text-container">.ipynb</span>
</a>
</li>
      
      
      
      
      <li>
<button onclick="window.print()"
  class="btn btn-sm btn-download-pdf-button dropdown-item"
  title="Print to PDF"
  data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file-pdf"></i>
  </span>
<span class="btn__text-container">.pdf</span>
</button>
</li>
      
  </ul>
</div>




<button onclick="toggleFullScreen()"
  class="btn btn-sm btn-fullscreen-button"
  title="Fullscreen mode"
  data-bs-placement="bottom" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-expand"></i>
  </span>

</button>


<script>
document.write(`
  <button class="theme-switch-button btn btn-sm btn-outline-primary navbar-btn rounded-circle" title="light/dark" aria-label="light/dark" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="theme-switch" data-mode="light"><i class="fa-solid fa-sun"></i></span>
    <span class="theme-switch" data-mode="dark"><i class="fa-solid fa-moon"></i></span>
    <span class="theme-switch" data-mode="auto"><i class="fa-solid fa-circle-half-stroke"></i></span>
  </button>
`);
</script>

<script>
document.write(`
  <button class="btn btn-sm navbar-btn search-button search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass"></i>
  </button>
`);
</script>
<label class="sidebar-toggle secondary-toggle btn btn-sm" for="__secondary"title="Toggle secondary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="fa-solid fa-list"></span>
</label>
</div></div>
      
    </div>
  
</div>
</div>
              
              

<div id="jb-print-docs-body" class="onlyprint">
    <h1>Introducción a la probabilidad</h1>
    <!-- Table of contents -->
    <div id="print-main-content">
        <div id="jb-print-toc">
            
            <div>
                <h2> Contents </h2>
            </div>
            <nav aria-label="Page">
                <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#como-de-diferentes-son-la-probabilidad-y-la-estadistica-probstats">¿Cómo de diferentes son la probabilidad y la estadística?{#probstats}</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#que-significa-la-probabilidad-probmeaning">¿Qué significa la probabilidad?{#probmeaning}</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#la-vision-frecuentista">La visión frecuentista</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#la-vision-bayesiana">La visión bayesiana</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#cual-es-la-diferencia-y-quien-tiene-razon">¿Cuál es la diferencia? ¿Y quién tiene razón?</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#teoria-de-probabilidad-basica-basicprobability">Teoría de probabilidad básica{#basicprobability}</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#introduccion-a-las-distribuciones-de-probabilidad">Introducción a las distribuciones de probabilidad</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#la-distribucion-binomial-binomial">La distribución binomial{#binomial}</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#introduccion-al-binomio">Introducción al binomio</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#la-distribucion-normal-normal">La distribución normal{#normal}</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#densidad-de-probabilidad-density">Densidad de probabilidad{#density}</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#otras-distribuciones-utiles-otherdists">Otras distribuciones útiles{#otherdists}</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#resumen">Resumen</a></li>
</ul>
            </nav>
        </div>
    </div>
</div>

              
                
<div id="searchbox"></div>
                <article class="bd-article" role="main">
                  
  <div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-r notranslate"><div class="highlight"><pre><span></span><span class="c1"># install.packages(&quot;ggplot2&quot;)</span>
<span class="nf">library</span><span class="p">(</span><span class="n">ggplot2</span><span class="p">)</span>

<span class="c1"># Load the data</span>
<span class="nf">data</span><span class="p">(</span><span class="n">mtcars</span><span class="p">)</span>
<span class="n">df</span> <span class="o">&lt;-</span> <span class="n">mtcars</span><span class="p">[,</span> <span class="nf">c</span><span class="p">(</span><span class="s">&quot;mpg&quot;</span><span class="p">,</span> <span class="s">&quot;cyl&quot;</span><span class="p">,</span> <span class="s">&quot;wt&quot;</span><span class="p">)]</span>
<span class="c1"># Convert cyl to a factor variable</span>
<span class="n">df</span><span class="o">$</span><span class="n">cyl</span> <span class="o">&lt;-</span> <span class="nf">as.factor</span><span class="p">(</span><span class="n">df</span><span class="o">$</span><span class="n">cyl</span><span class="p">)</span>
<span class="nf">head</span><span class="p">(</span><span class="n">df</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_html"><table class="dataframe">
<caption>A data.frame: 6 × 3</caption>
<thead>
	<tr><th></th><th scope=col>mpg</th><th scope=col>cyl</th><th scope=col>wt</th></tr>
	<tr><th></th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;fct&gt;</th><th scope=col>&lt;dbl&gt;</th></tr>
</thead>
<tbody>
	<tr><th scope=row>Mazda RX4</th><td>21.0</td><td>6</td><td>2.620</td></tr>
	<tr><th scope=row>Mazda RX4 Wag</th><td>21.0</td><td>6</td><td>2.875</td></tr>
	<tr><th scope=row>Datsun 710</th><td>22.8</td><td>4</td><td>2.320</td></tr>
	<tr><th scope=row>Hornet 4 Drive</th><td>21.4</td><td>6</td><td>3.215</td></tr>
	<tr><th scope=row>Hornet Sportabout</th><td>18.7</td><td>8</td><td>3.440</td></tr>
	<tr><th scope=row>Valiant</th><td>18.1</td><td>6</td><td>3.460</td></tr>
</tbody>
</table>
</div></div>
</div>
<table class="table" id="example-table">
<caption><span class="caption-text">This table title</span><a class="headerlink" href="#example-table" title="Permalink to this table">#</a></caption>
<thead>
<tr class="row-odd"><th class="head"><p>Training</p></th>
<th class="head"><p>Validation</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>0</p></td>
<td><p>5</p></td>
</tr>
<tr class="row-odd"><td><p>13720</p></td>
<td><p>2744</p></td>
</tr>
</tbody>
</table>
<section class="tex2jax_ignore mathjax_ignore" id="introduccion-a-la-probabilidad">
<h1>Introducción a la probabilidad<a class="headerlink" href="#introduccion-a-la-probabilidad" title="Permalink to this heading">#</a></h1>
<p>Para muchas personas, cuando piensan en estadística se les viene esto a la mente: calcular promedios, recopilar datos, elaborar gráficos y ponerlos todos en un informe en algún lugar. Mas o menos como coleccionar sellos o cucharillas, pero con números. Sin embargo, las estadística cubre mucho más que eso. De hecho, la estadística descriptiva es una de las partes más pequeñas de la estadística, y una de las menos poderosas (en cuanto a las conclusiones que puede aportar). La parte más importante y más útil de la estadística es aquella que que permite hacer <em>inferencias</em>  sobre los datos.</p>
<p>Una vez contemplada las estadística en estos términos, -que la estadística está ahí para ayudarnos a hacer inferencias a partir de datos- podemos ver ejemplos de ella en todas partes. Por ejemplo, aquí hay un pequeño extracto de un periódico mexicano:</p>
<blockquote>
<div><p>“Tengo un trabajo difícil”, dijo el Presidente mexicano en respuesta a una encuesta que encontró que su gobierno ha pasado de gozar los mayores índices de popularidad en la historia (&gt;70%) a un 38 por ciento.
Este tipo de comentarios suele pasar como completamente irrelevante en los periódicos o en la vida cotidiana, pero pensemos brevemento sobre lo que implica. Una compañía encuestadora ha realizado una encuesta, presumiblemente una muy grande porque tiene los medios y puede permitírselo. Imaginemos que llamaron a 1.000 votantes al azar, y 380 (38%) de ellos afirmaron que tenían la intención de votar por el Presidente. En las últimas elecciones federals, la Instituto Electoral Mexicano confirmó la participación de 56.611.027 votantes; por lo tanto, las opiniones de los 56.610.027 votantes restantes (aproximadamente el 99.998%  de los votantes) siguen siendo desconocidas para la encuestadora (y para nosotros). Aún suponiendo que nadie mintió en la encuesta, lo único que podemos decir con un 100% de seguridad es que el verdadero voto primario al Presidente está en algún lugar entre 380/56.611.027 (aproximadamente el 0.0007%) y 56.610.307/56.611.027 (alrededor del 99.9987%), lo cual no aporta mucho. Entonces, ¿sobre qué base es legítimo para la empresa encuestadora, el periódico y el lectores concluir que la intención de voto al Presidente mexicano fue de sólo el 38%?</p>
</div></blockquote>
<p>La respuesta a la pregunta es bastante obvia: si llamo a 1.000 personas al azar, y 380 de ellas dicen tienen la intención de votar por el Presidente, entonces parece muy poco probable que estas sean las <em>únicas</em> 380 personas del todo el público votante que realmente tiene la intención de hacerlo. En otras palabras, suponemos que los datos recopilados por la empresa encuestadora son bastante representativos de la población en general. ¿Pero qué tan representativo? ¿Nos sorprendería descubrir que la verdadera intención de voto es en realidad del 34%? ¿39%? ¿57%? En este punto nuestración intuición comienza a romperse un poco. Nadie se sorprendería si fuese el 34%, y todos lo harían con un 57%, pero es un poco difícil decir si el 39% es plausible. Necesitamos herramientas más poderosas que solo el mirar los números y adivinar.</p>
<p><strong><em>La estadística inferencial</em></strong>  proporciona las herramientas que necesitamos para responder a este tipo de preguntas, y ya que este tipo de preguntas se encuentran en el corazón del quehacer científico, ocupan una parte sustancial de cada curso introductorio sobre estadística y métodos de investigación. Sin embargo, la teoría sobre la estadística inferencial está construida sobre la <strong><em>teoría de probabilidad</em></strong>.  Y es a la teoría de la probabilidad a la que ahora debemos prestar atención. La discusión de la teoría de la probabilidad en esta asignatura será básicamente de fondo: no hay mucho contenido estadístico <em>per se</em> en este capítulo. Sin embargo, debido a que gran parte de la estadística se sustenta en la teoría de la probabilidad, merece la pena ir cubriendo algunos de los conceptos básicos.</p>
<section id="como-de-diferentes-son-la-probabilidad-y-la-estadistica-probstats">
<h2>¿Cómo de diferentes son la probabilidad y la estadística?{#probstats}<a class="headerlink" href="#como-de-diferentes-son-la-probabilidad-y-la-estadistica-probstats" title="Permalink to this heading">#</a></h2>
<p>Antes de comenzar a hablar sobre la teoría de la probabilidad, es útil pensar un momento en la relación que existe entre probabilidad y estadística. Las dos disciplinas están estrechamente relacionadas, pero no son idénticas. La teoría de la probabilidad es “la doctrina de las posibilidades”. Es una rama de las matemáticas que te dice con qué frecuencia sucederán diferentes tipos de eventos. Por ejemplo, todas estas preguntas son cosas que puedes responder usando la teoría de la probabilidad:</p>
<ul class="simple">
<li><p>¿Cuáles son las probabilidades de que al lanzar una moneda salga cara 10 veces seguidas?</p></li>
<li><p>Si lanzo dos dados de seis caras, ¿qué probabilidad hay de que tire dos seises?</p></li>
<li><p>¿Qué probabilidad hay de que cinco cartas extraídas de un mazo perfectamente barajado sean todas de corazones?</p></li>
<li><p>¿Cuál es la probabilidad de que gane la lotería?</p></li>
</ul>
<p>Hay que tener en cuenta que todas estas preguntas tienen algo en común. En cada caso, existe y se conoce una “verdad sobre el mundo”, y la pregunta se refiere más bien al “qué tipo de eventos” sucederán. En la primera pregunta que <em>sé</em> que la moneda es justa (no es más pesada por uno de los lados, sesgando el resultado), por lo que hay un 50% de probabilidad de que cualquier lanzamiento de moneda salga cara. En la segunda pregunta, <em>sé</em> que la probabilidad de sacar un 6 en un solo dado es de 1 en 6. En la tercera pregunta <em>sé</em> que la baraja se baraja correctamente (no hay un acomodo de cartas). Y en la cuarta pregunta, <em>sé</em> que la lotería sigue unas reglas específicas. El punto clave aquí es que las preguntas probabilísticas comienzan con un <strong><em>modelo</em></strong> conocido del mundo, y usamos ese modelo para hacer algunos cálculos. El modelo subyacente puede ser bastante simple. Por ejemplo, en el ejemplo de lanzar monedas, podemos escribir el modelo de esta manera:
$<span class="math notranslate nohighlight">\(
P(\mbox{cara}) = 0.5
\)</span>$
que puedes leer como “la probabilidad de que salga cara es 0.5”. Como veremos más adelante, de la misma manera que los porcentajes son números que van del 0% al 100%, las probabilidades son solo números que van del 0 al 1. Cuando usamos este modelo de probabilidad para responder a la primera pregunta, en realidad no sé exactamente qué va a ocurrir. Tal vez obtenga 10 caras, como dice la pregunta. Pero quizás salgan sólo tres caras. Esta es la clave: con la teoría de la probabilidad, se conoce el <em>modelo</em>, pero no los <em>datos</em>.</p>
<p>Hemos visto lo que es la probabilidad. ¿Qué hay de la estadística? Las preguntas en estadística funcionan al revés. En estadística, nosotros <em>no</em> sabemos la verdad sobre el mundo. Todo lo que tenemos son los datos, y es a partir de esos datos que queremos <em>aprender</em> sobre la verdad del mundo. Las preguntas estadísticas tienden a parecerse más a estas:</p>
<ul class="simple">
<li><p>Si mi amigo lanza una moneda 10 veces y obtiene 10 caras, ¿me está engañando?</p></li>
<li><p>Si las primeras cinco cartas de la parte superior del mazo son todas de corazones, ¿qué tan probable es que se haya barajado el mazo?</p></li>
<li><p>Si el hijo del comisionado de la lotería gana la lotería, ¿qué tan probable es que el sorteo esté amañado?</p></li>
</ul>
<p>Esta vez, lo único que tenemos son datos. Lo que <em>sé</em>  es que vi a mi amigo lanzar la moneda 10 veces y salió cara en cada una de las veces. Y lo que quiero es <strong><em>inferir</em></strong> si debería concluir que lo que acabo de ver es en realidad una moneda justa lanzada 10 veces seguidas, o si debería sospechar que mi amigo me está jugando una mala pasada. Los datos que tengo se ven así (cada C es una cara):</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">C</span> <span class="n">C</span> <span class="n">C</span> <span class="n">C</span> <span class="n">C</span> <span class="n">C</span> <span class="n">C</span> <span class="n">C</span> <span class="n">C</span> <span class="n">C</span> <span class="n">C</span>
</pre></div>
</div>
<p>y lo que estoy tratando de hacer es averiguar en qué “modelo de verdad del mundo” debería confiar. Si la moneda es justa, entonces el modelo que debo aceptar es uno que diga que la probabilidad de que salga cara es 0.5; es decir,  <span class="math notranslate nohighlight">\(P(\mbox{cara}) = 0.5\)</span>. Si la moneda no es justa, entonces debo concluir que la probabilidad de que salga cara <em>no</em> es 0.5, lo cual escribiríamos como <span class="math notranslate nohighlight">\(P(\mbox{cara}) \neq 0.5\)</span>. En otras palabras, el objetivo de la inferencia estadística es decidir cual de estos modelos de probabilidad es el correcto. Vemos pues, que una pregunta en estadística no es la misma que una pregunta en probabilidad, pero están íntimamente conectados entre sí. Es por ello que una buena introducción a la teoría estadística comenzará con una discusión sobre qué es la probabilidad y cómo funciona.</p>
</section>
<section id="que-significa-la-probabilidad-probmeaning">
<h2>¿Qué significa la probabilidad?{#probmeaning}<a class="headerlink" href="#que-significa-la-probabilidad-probmeaning" title="Permalink to this heading">#</a></h2>
<p>Comencemos con la primera de estas preguntas. ¿Qué es la “probabilidad”? Puede parecer sorprendente, pero mientras que los estadísticos y matemáticos (en su mayoría) están de acuerdo sobre cuáles son las <em>reglas</em> de la probabilidad, hay mucho menos consenso sobre lo que realmente <em>significa</em> la palabra. Parece extraño porque todos usemos con soltura palabras como “posibilidad”, “probabilidad”, “posible” y “probable”, y además no parece que deba ser una pregunta difícil de responder. Si tuvieramos que explicar el concepto de “probabilidad” a un niño de cinco años, podríamos hacerlo sin muchos problemas. Pero si alguna vez lo has intentando en la vida real, podrías terminar esa conversación sintiendo que no lo has hecho muy bien y que (como con muchos conceptos cotidianos) resulta que <em>realmente</em> no sabemos de qué se trata.</p>
<section id="la-vision-frecuentista">
<h3>La visión frecuentista<a class="headerlink" href="#la-vision-frecuentista" title="Permalink to this heading">#</a></h3>
<p>El primero de los dos enfoques principales a la teoría de la probabilidad, y el más dominante en estadística, se le conoce como la <strong><em>visión frecuentista</em></strong>, y define a la probabilidad como una <strong>_ frecuencia a largo plazo_</strong>. Supongamos que queremos intentar lanzar una moneda justa, una y otra vez. Por definición, esta es una moneda que tiene una <span class="math notranslate nohighlight">\(P(Cara) = 0.5\)</span>. ¿Qué resultado podremos observar? Una posibilidad es que los primeros 20 lanzamientos se vean así (donde C es cara y X cruz):</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-r notranslate"><div class="highlight"><pre><span></span><span class="n">```</span>
<span class="n">X,C,C,C,C,X,X,C,C,C,C,X,C,C,X,X,X,X,X,C</span>
<span class="n">```</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output traceback highlight-ipythontb notranslate"><div class="highlight"><pre><span></span><span class="n">Error</span> <span class="ow">in</span> <span class="n">parse</span><span class="p">(</span><span class="n">text</span> <span class="o">=</span> <span class="n">x</span><span class="p">,</span> <span class="n">srcfile</span> <span class="o">=</span> <span class="n">src</span><span class="p">):</span> <span class="n">attempt</span> <span class="n">to</span> <span class="n">use</span> <span class="n">zero</span><span class="o">-</span><span class="n">length</span> <span class="n">variable</span> <span class="n">name</span>
<span class="ne">Traceback</span>:
</pre></div>
</div>
</div>
</div>
<p>X,C,C,C,C,X,X,C,C,C,C,X,C,C,X,X,X,X,X,C</p>
<p>En este caso, en 11 de los 20 lanzamientos (55%) salió cara. Ahora supongamos que he ido guardando un registro con el número de caras (que llamaré <span class="math notranslate nohighlight">\(N_C\)</span>) que han salido, a lo largo de las primeras <span class="math notranslate nohighlight">\(N\)</span> lanzadas de moneda, además de calcular la proporción de caras <span class="math notranslate nohighlight">\(N_C / N\)</span> con cada registro. Este es el resultado que obtendría:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-r notranslate"><div class="highlight"><pre><span></span><span class="n">knitr</span><span class="o">::</span><span class="nf">kable</span><span class="p">(</span><span class="nf">data.frame</span><span class="p">(</span><span class="n">stringsAsFactors</span><span class="o">=</span><span class="kc">FALSE</span><span class="p">,</span>
<span class="n">`Número de lanzamientos`</span> <span class="o">=</span> <span class="nf">c</span><span class="p">(</span><span class="m">1</span> <span class="p">,</span> <span class="m">2</span> <span class="p">,</span> <span class="m">3</span> <span class="p">,</span> <span class="m">4</span> <span class="p">,</span> <span class="m">5</span> <span class="p">,</span> <span class="m">6</span> <span class="p">,</span> <span class="m">7</span> <span class="p">,</span> <span class="m">8</span> <span class="p">,</span> <span class="m">9</span> <span class="p">,</span> <span class="m">10</span><span class="p">,</span> <span class="m">11</span> <span class="p">,</span> <span class="m">12</span> <span class="p">,</span> <span class="m">13</span> <span class="p">,</span> <span class="m">14</span> <span class="p">,</span> <span class="m">15</span> <span class="p">,</span> <span class="m">16</span> <span class="p">,</span> <span class="m">17</span> <span class="p">,</span> <span class="m">18</span> <span class="p">,</span> <span class="m">19</span> <span class="p">,</span> <span class="m">20</span><span class="p">),</span>
<span class="n">`Número de caras`</span> <span class="o">=</span> <span class="nf">c</span><span class="p">(</span> <span class="m">0</span> <span class="p">,</span> <span class="m">1</span> <span class="p">,</span> <span class="m">2</span> <span class="p">,</span> <span class="m">3</span> <span class="p">,</span> <span class="m">4</span> <span class="p">,</span> <span class="m">4</span> <span class="p">,</span> <span class="m">4</span> <span class="p">,</span> <span class="m">5</span> <span class="p">,</span> <span class="m">6</span> <span class="p">,</span> <span class="m">7</span><span class="p">,</span>  <span class="m">8</span>  <span class="p">,</span>  <span class="m">8</span> <span class="p">,</span>  <span class="m">9</span> <span class="p">,</span> <span class="m">10</span> <span class="p">,</span> <span class="m">10</span> <span class="p">,</span> <span class="m">10</span> <span class="p">,</span> <span class="m">10</span> <span class="p">,</span> <span class="m">10</span> <span class="p">,</span> <span class="m">10</span> <span class="p">,</span> <span class="m">11</span> <span class="p">),</span> 
<span class="n">`Proporción`</span> <span class="o">=</span> <span class="nf">c</span><span class="p">(</span> <span class="n">.</span><span class="m">00</span> <span class="p">,</span> <span class="n">.</span><span class="m">50</span> <span class="p">,</span> <span class="n">.</span><span class="m">67</span> <span class="p">,</span> <span class="n">.</span><span class="m">75</span> <span class="p">,</span> <span class="n">.</span><span class="m">80</span> <span class="p">,</span> <span class="n">.</span><span class="m">67</span> <span class="p">,</span> <span class="n">.</span><span class="m">57</span> <span class="p">,</span> <span class="n">.</span><span class="m">63</span> <span class="p">,</span> <span class="n">.</span><span class="m">67</span> <span class="p">,</span> <span class="n">.</span><span class="m">70</span><span class="p">,</span> <span class="n">.</span><span class="m">73</span> <span class="p">,</span> <span class="n">.</span><span class="m">67</span> <span class="p">,</span> <span class="n">.</span><span class="m">69</span> <span class="p">,</span> <span class="n">.</span><span class="m">71</span> <span class="p">,</span> <span class="n">.</span><span class="m">67</span> <span class="p">,</span>  <span class="n">.</span><span class="m">63</span> <span class="p">,</span> <span class="n">.</span><span class="m">59</span> <span class="p">,</span> <span class="n">.</span><span class="m">56</span> <span class="p">,</span> <span class="n">.</span><span class="m">53</span> <span class="p">,</span> <span class="n">.</span><span class="m">55</span><span class="p">)))</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>| Número.de.lanzamientos| Número.de.caras| Proporción|
|----------------------:|---------------:|----------:|
|                      1|               0|       0.00|
|                      2|               1|       0.50|
|                      3|               2|       0.67|
|                      4|               3|       0.75|
|                      5|               4|       0.80|
|                      6|               4|       0.67|
|                      7|               4|       0.57|
|                      8|               5|       0.63|
|                      9|               6|       0.67|
|                     10|               7|       0.70|
|                     11|               8|       0.73|
|                     12|               8|       0.67|
|                     13|               9|       0.69|
|                     14|              10|       0.71|
|                     15|              10|       0.67|
|                     16|              10|       0.63|
|                     17|              10|       0.59|
|                     18|              10|       0.56|
|                     19|              10|       0.53|
|                     20|              11|       0.55|
</pre></div>
</div>
</div>
</div>
<p>Tengamos en cuenta que al comienzo de esta secuencia, la <em>proporción</em> de caras fluctúa enormemente, comenzando en .00 y subiendo tan alto como .80. Conforme aumenta el número de lanzamientos, uno tiene la impresión de que este efecto se amortigua un poco, mientras que los valores se aproximan cada vez más a la respuesta “correcta” de .50. Esta es la definición frecuentista de probabilidad en pocas palabras: lanzar una moneda justa una y otra vez, y a medida que <span class="math notranslate nohighlight">\(N\)</span> crece  (se acerca al infinito, denotado como <span class="math notranslate nohighlight">\(N\rightarrow \infty\)</span>), la proporción de caras convergerá en el 50%. Tecnicismos matemáticos aparte, cualitativamente hablando, es así es como los frecuentistas definen la probabilidad. Desafortunadamente, no tengo un número infinito de monedas, o la paciencia infinita requerida para lanzar una moneda un número infinito de veces. Sin embargo, existen los ordenadores, y los ordenadores se destacan por la ejecución repetitiva de tareas sin sentido como esta. Entonces, al simular 1.000 lanzamientos de moneda y repetir este procesos 4 veces (para darle solidez a los resultados), podemos ver qué sucede con la proporción <span class="math notranslate nohighlight">\(N_C / N\)</span> a medida que <span class="math notranslate nohighlight">\(N\)</span> aumenta. Los resultados se muestran en la Figura &#64;ref(fig:frequentistprobability) aunque también puedes hacer tú la simulación haciendo click <a class="reference external" href="https://leudave.shinyapps.io/cara_cruz/">aquí</a>. Podemos apreciar que la <em>proporción de caras observadas</em> deja de fluctuar conforme aumenta el número de lanzamientos; cuando lo hace, el número que finalmente obtenemos es el verdadera probabilidad de salga cara.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-r notranslate"><div class="highlight"><pre><span></span><span class="n">emphCol</span> <span class="o">&lt;-</span> <span class="nf">rgb</span><span class="p">(</span><span class="m">0</span><span class="p">,</span><span class="m">0</span><span class="p">,</span><span class="m">1</span><span class="p">)</span>
<span class="n">emphColLight</span> <span class="o">&lt;-</span> <span class="nf">rgb</span><span class="p">(</span><span class="n">.</span><span class="m">5</span><span class="p">,</span><span class="n">.</span><span class="m">5</span><span class="p">,</span><span class="m">1</span><span class="p">)</span>
<span class="n">emphGrey</span> <span class="o">&lt;-</span> <span class="nf">grey</span><span class="p">(</span><span class="n">.</span><span class="m">5</span><span class="p">)</span>
<span class="n">eps</span> <span class="o">&lt;-</span> <span class="kc">TRUE</span>
<span class="n">colour</span> <span class="o">&lt;-</span> <span class="kc">TRUE</span>
	<span class="n">width</span> <span class="o">&lt;-</span> <span class="m">12</span>
	<span class="n">height</span> <span class="o">&lt;-</span> <span class="m">8</span>
	<span class="n">def.par</span> <span class="o">&lt;-</span> <span class="nf">par</span><span class="p">(</span><span class="n">no.readonly</span> <span class="o">=</span> <span class="kc">TRUE</span><span class="p">)</span>
	<span class="nf">layout</span><span class="p">(</span> <span class="nf">matrix</span><span class="p">(</span><span class="m">1</span><span class="o">:</span><span class="m">4</span><span class="p">,</span><span class="m">2</span><span class="p">,</span><span class="m">2</span><span class="p">)</span> <span class="p">)</span>
	<span class="nf">for</span><span class="p">(</span> <span class="n">i</span> <span class="n">in</span> <span class="m">1</span><span class="o">:</span><span class="m">4</span> <span class="p">)</span> <span class="p">{</span>
		
		<span class="n">X</span> <span class="o">&lt;-</span> <span class="nf">as.numeric</span><span class="p">(</span> <span class="nf">runif</span><span class="p">(</span><span class="m">1000</span><span class="p">)</span> <span class="o">&gt;</span> <span class="n">.</span><span class="m">5</span> <span class="p">)</span>
		<span class="n">X</span> <span class="o">&lt;-</span> <span class="nf">cumsum</span><span class="p">(</span><span class="n">X</span><span class="p">)</span> <span class="o">/</span> <span class="p">(</span><span class="m">1</span><span class="o">:</span><span class="m">1000</span><span class="p">)</span>
		<span class="nf">plot</span><span class="p">(</span> <span class="m">1</span><span class="o">:</span><span class="m">1000</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">type</span><span class="o">=</span><span class="s">&quot;l&quot;</span><span class="p">,</span> <span class="n">ylim</span><span class="o">=</span><span class="nf">c</span><span class="p">(</span><span class="n">.</span><span class="m">3</span><span class="p">,</span><span class="n">.</span><span class="m">7</span><span class="p">),</span> <span class="n">col</span><span class="o">=</span><span class="nf">ifelse</span><span class="p">(</span><span class="n">colour</span><span class="p">,</span><span class="n">emphCol</span><span class="p">,</span><span class="n">emphGrey</span><span class="p">),</span>
			<span class="n">xlab</span> <span class="o">=</span> <span class="s">&quot;Número de lanzadas&quot;</span><span class="p">,</span> <span class="n">ylab</span> <span class="o">=</span> <span class="s">&quot;Proporción de caras&quot;</span><span class="p">,</span> <span class="n">lwd</span><span class="o">=</span><span class="m">3</span>
			<span class="p">)</span> 
		<span class="nf">abline</span><span class="p">(</span><span class="n">h</span><span class="o">=</span><span class="n">.</span><span class="m">5</span><span class="p">,</span><span class="n">lty</span><span class="o">=</span><span class="m">2</span><span class="p">,</span><span class="n">col</span><span class="o">=</span><span class="n">emphGrey</span><span class="p">,</span><span class="n">lwd</span><span class="o">=</span><span class="m">2</span><span class="p">)</span>
		
	<span class="p">}</span>
	<span class="nf">par</span><span class="p">(</span><span class="n">def.par</span><span class="p">)</span><span class="c1">#- reset to default</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/a9bb47f0d1a006fc430a2d41c17050077eecd89b810dd2b6742b4bc536e22a67.png" src="../_images/a9bb47f0d1a006fc430a2d41c17050077eecd89b810dd2b6742b4bc536e22a67.png" />
</div>
</div>
<p>La definición frecuentista de probabilidad tiene algunas características que le hacen deseable. En primer lugar, es objetivo: la probabilidad de un evento se basa <em>necesariamente</em> en el mundo real. La única forma en que declaraciones de probabilidad puedan tener sentido es si se refieren a (una secuencia de) eventos que ocurren en el universo físico. En segundo lugar, es inequívoco: dos personas que miran como se desarrolla la misma secuencia de eventos, al tratar de calcular la probabilidad de un evento, inevitablemente deberán llegar a la misma respuesta. Sin embargo, también tiene algunas características no tan deseables. En primer lugar, no existen secuencias infinitas en el mundo físico. Supongamos que has encontrado una moneda en el suelo y has comenzado a lanzarla varias veces. Cada vez que aterriza, impacta en el suelo. Cada impacto daña un poco la moneda; eventualmente, la moneda será inutilizable. Entonces, uno podría preguntarse si realmente tiene sentido fingir que una secuencia “infinita” de lanzamientos de monedas es un concepto significativo u objetivo. No podemos decir que una “secuencia infinita” de eventos es algo real en el universo físico, porque el universo físico no permite nada infinito. Así, podemos ver que la definición frecuentista tiene un alcance limitado. Hay muchas cosas por ahí a las que los seres humanos estamos felices de asignar probabilidades en el día a día, pero que no pueden (ni siquiera en teoría) ser planteadas como una secuencia hipotética de eventos. Por ejemplo, si un meteorólogo aparece en televisión y dice: “la probabilidad de lluvia en Pamplona el 2 de noviembre del 2048 es del 60%” nosotros podemos aceptarlo sin rechistar. Pero desde un punto de vista frecuentista, no queda tan claro cómo podemos definirlo. Sólo hay una ciudad de Pamplona (en Navarra), y sólo un 2 de noviembre del 2048. Aquí no hay una secuencia infinita de eventos, solo una cosa de una vez. La probabilidad frecuentista nos <em>prohíbe</em> hacer declaraciones de probabilidad sobre un solo evento. Desde la perspectiva frecuentista, lloverá mañana o o no lloverá; no hay una “probabilidad” que se pueda adjuntar a un sólo evento no repetible. Sin embargo, existen algunos trucos que los frecuentistas pueden utilizar para solucionar esta situación. Una posibilidad es que el meteorólogo en realidad nos quiera decir algo así: “Existe una categoría de días para la que predigo un 60% probabilidad de lluvia; si miramos sólo esos días para los que hago esta predicción, entonces en el 60%  de esos días lloverá realmente”. Es un poco extraño y contradictorio pensarlo de esta manera, pero los frecuentistas hacen esto a veces.</p>
</section>
<section id="la-vision-bayesiana">
<h3>La visión bayesiana<a class="headerlink" href="#la-vision-bayesiana" title="Permalink to this heading">#</a></h3>
<p>La alternativa a la visión frecuentista, la <strong><em>visión bayesiana</em></strong> de la probabilidad, a menudo se denomina visión subjetivista, y es una visión relativamente minoritaria entre los estadísticos, aunque ha ido ganando terreno constantemente a lo largo de las últimas décadas. Hay muchos formas de bayesianismo, lo que hace difícil decir exactamente cuál es “la” visión bayesiana. La forma más fácil de entender la probabilidad subjetiva es al definir la probabilidad de un evento como el <strong><em>grado de creencia</em></strong> que un agente inteligente y racional  asigna a la verdad de ese evento. Desde esa perspectiva, las probabilidades no existen en el mundo, sino más bien en los pensamientos y suposiciones de las personas y otros seres inteligentes. Sin embargo, para que este enfoque funcione, necesitamos una forma de operacionalizar este “grado de creencia”. Una forma de hacerlo es formalizándolo en términos de una “apuesta racional” aunque existen muchas otras formas. Supongamos que creo que existe una probabilidad de que llueva mañana de un 60%. Si alguien me hace una apuesta, si llueve mañana, gano $5,  pero si no llueve, pierdo $5. Claramente, desde mi punto de vista, esta es una buena apuesta. Por otro lado, si creo que la probabilidad de lluvia es sólo del 40%, entonces es una mala apuesta. Por lo tanto, podemos poner en práctica la noción de una “probabilidad subjetiva” en términos de qué apuestas que estoy dispuesto a aceptar.</p>
<p>¿Cuáles son las ventajas y desventajas del enfoque bayesiano? La principal ventaja es que le permite asignar probabilidades a cualquier evento. No esta limitado a aquellos eventos que son repetibles. La principal desventaja (para muchas personas) es que no podemos ser realmente objetivos - especificar una probabilidad requiere que especifiquemos la entidad que tiene el grado de creencia que estamos examinando. Esta entidad puede ser un humano, un extraterrestre, un robot o incluso un estadístico, pero tiene que ser un agente inteligente que sea capaz de creer en cosas. Para muchas personas esto representa un inconveniente: parece hacer que la probabilidad sea arbitraria. Si bien el enfoque bayesiano requiere que el agente en cuestión sea racional (es decir, que obedezca las reglas de la probabilidad), permite que todos tengan sus propias creencias; yo puedo creer que la moneda es justa mientras que otro no, aunque ambos seamos racionales. La visión frecuentista no permite que dos observadores atribuyan diferentes probabilidades al mismo evento: cuando eso sucede, al menos uno de ellos debe estar equivocado. La visión bayesiana no evita que esto ocurra. Dos observadores con diferentes conocimientos previos pueden tener creencias diferentes sobre el mismo evento. En otras palabras, mientras que la visión frecuentista se puede considerar como demasiado estrecha (prohíbe muchas cosas a las cuales queremos asignar probabilidades), la visión bayesiana puede resultar demasiado amplia (permite demasiadas diferencias entre observadores).</p>
</section>
<section id="cual-es-la-diferencia-y-quien-tiene-razon">
<h3>¿Cuál es la diferencia? ¿Y quién tiene razón?<a class="headerlink" href="#cual-es-la-diferencia-y-quien-tiene-razon" title="Permalink to this heading">#</a></h3>
<p>Ahora que hemos visto ambas visiones estadísticas de forma independiente, es necesario compararlas. Regresemos al hipotético juego de fútbol de robots que comentamos comienzo del tema. ¿Que dirían un frecuentista y un bayesiano sobre las tres afirmaciones? ¿Qué enunciado sería la definición de probabilidad correcta para el frecuentista? ¿Y para el bayesiano? ¿Es posible que alguno de los enunciados no tenga sentido para cualquiera de los dos? Si entendemos ambas perspectivas, podemos intuir cómo responder a estas preguntas.</p>
<p>Entendiendo las diferencias, podemos preguntarnos a continuación cuál de dos enfoques es el <em>correcto</em>. Sin embargo, no existe una respuesta correcta. Matemáticamente hablando, no hay nada incorrecto sobre la forma en que los frecuentistas piensan sobre secuencias de eventos, ni hay nada incorrecto acerca de la forma en que los bayesianos definen las creencias de un agente racional. De hecho, si vamos al detalle, los bayesianos y los frecuentistas en realidad están de acuerdo en muchas cosas. Muchos métodos frecuentistas conducen a decisiones que los bayesianos pensarían que toma un agente racional. Muchos métodos bayesianos tienen buenas propiedades frecuentistas.</p>
<p>En cualquier caso, la mayor parte de los métodos y análisis estadísticos en la literatura se basan en el enfoque frecuentista. Por lo tanto, el objetivo de esta asignatura es cubrir aproximadamente el mismo temario que una clase típica de estadística de pregrado en ciencias de la educación, y si queremos entender las herramientas estadísticas utilizadas por la mayoría de los educadores en investigación, necesitaremos una buena comprensión de los métodos frecuentistas.</p>
</section>
</section>
<section id="teoria-de-probabilidad-basica-basicprobability">
<h2>Teoría de probabilidad básica{#basicprobability}<a class="headerlink" href="#teoria-de-probabilidad-basica-basicprobability" title="Permalink to this heading">#</a></h2>
<p>A pesar de los argumentos ideológicos entre bayesianos y frecuentistas, existe un consenso más o menos generalizado sobre las reglas que la probabilidad debe obedecer. Hay muchas formas de abordar estas reglas. El enfoque más utilizado se basa en el trabajo de Andrey Kolmogorov, uno de los grandes matemáticos soviéticos del siglo XX. No entraremos mucho en detalle, pero aprenderemos en qué consisten y cómo utilizarlas a través del siguiente ejemplo.</p>
<section id="introduccion-a-las-distribuciones-de-probabilidad">
<h3>Introducción a las distribuciones de probabilidad<a class="headerlink" href="#introduccion-a-las-distribuciones-de-probabilidad" title="Permalink to this heading">#</a></h3>
<p>Un hecho comprobado sobre mi vida es que sólo tengo 5 pares de pantalones: tres pares de vaqueros, los pantalones de un traje y un par de pantalones de chándal. Lo más triste es que les he dado nombres: los llamo <span class="math notranslate nohighlight">\(X_1\)</span>, <span class="math notranslate nohighlight">\(X_2\)</span>, <span class="math notranslate nohighlight">\(X_3\)</span>, <span class="math notranslate nohighlight">\(X_4\)</span> y <span class="math notranslate nohighlight">\(X_5\)</span>. Diariamente, por la mañana, elijo un único par de esos pantalones que voy a usar. Si yo tuviera que describir esta situación usando el lenguaje de la teoría de la probabilidad, me referiría a cada par de pantalones (es decir, a cada <span class="math notranslate nohighlight">\(X\)</span>) como un <strong><em>evento elemental</em></strong>.  La característica clave de estos eventos elementales es que cada vez que hacemos una observación (por ejemplo, cada vez que escojo un par de pantalones), el resultado será uno y solo uno de estos eventos. Como he dicho antes, siempre uso exactamente sólo un par de pantalones, así que mis pantalones cumplen con esta restricción. Del mismo modo, al conjunto de todos los eventos posibles se le denomina  <strong><em>espacio muestral</em></strong>. Siguiendo con el ejemplo, mi espacio muestral sería el armario que contiene los 5 pantalones.</p>
<p>Bien, ahora que tenemos un espacio muestral (un armario), que está construido a partir de muchas posibles eventos elementales (pantalones), lo que queremos hacer es asignar una <strong><em>probabilidad</em></strong>  a cada uno de estos eventos elementales. Para un evento <span class="math notranslate nohighlight">\(X\)</span>, la probabilidad de ese evento <span class="math notranslate nohighlight">\(P(X)\)</span> es un número que se encuentra entre 0 y 1. Cuanto mayor sea el valor de <span class="math notranslate nohighlight">\(P(X)\)</span>, más probable será que ocurra el evento. Entonces, por ejemplo, si <span class="math notranslate nohighlight">\(P(X) = 0\)</span>, significa que el evento <span class="math notranslate nohighlight">\(X\)</span> es imposible (es decir, nunca uso esos pantalones). Por otro lado, si  <span class="math notranslate nohighlight">\(P(X) = 1\)</span> significa que el evento <span class="math notranslate nohighlight">\(X\)</span> es seguro que ocurra (es decir, siempre uso esos pantalones). Los valores de probabilidad intermedios, significan que a veces uso esos pantalones (y a veces no). Por ejemplo, una <span class="math notranslate nohighlight">\(P(X) = 0.5\)</span> significa que uso esos pantalones la mitad de las veces.</p>
<p>Llegados a este punto, lo siguiente que debemos entender es que “algo siempre sucede “. Cada vez que me pongo unos pantalones, realmente termino usando esos pantalones. Lo que esto significa en términos probabilísticos, es que las probabilidades de todos los eventos elementales siempre suman 1. Esto se conoce como la <strong><em>ley de probabilidad total</em></strong>. Si se cumplen estos requisitos (tenemos número <span class="math notranslate nohighlight">\(X\)</span> de pantalones, cada par con una probabilidad <span class="math notranslate nohighlight">\(P(X)\)</span> de usarlos que en total suman 1), entonces lo que tenemos es una <strong><em>distribución de probabilidad</em></strong>.Veamos un ejemplo de distribución de probabilidad.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-r notranslate"><div class="highlight"><pre><span></span><span class="n">```{r echo=FALSE}</span>
<span class="n">knitr::kable(data.frame(stringsAsFactors=FALSE,</span>
<span class="n">`Pantalones` = c(  &quot;Nombre&quot; , &quot;Probabilidad&quot; ), </span>
<span class="n">`V.</span> <span class="n">azules` = c(  &quot;$X_1$&quot; , &quot;$P(X_1) = .5$&quot; ),</span>
<span class="n">`V.</span> <span class="n">grises` = c(  &quot;$X_2$ &quot;, &quot;$P(X_2) = .3$ &quot;),</span>
<span class="n">`V.</span> <span class="n">negros` = c(  &quot;$X_3$ &quot;, &quot;$P(X_3) = .1$&quot; ),</span>
<span class="n">`Traje</span> <span class="n">negro` = c(  &quot;$X_4$&quot; , &quot;$P(X_4) = 0$ &quot;),</span>
<span class="n">`Chándal</span> <span class="n">azul` = c(  &quot;$X_5$ &quot;, &quot;$P(X_5) = .1$&quot;)), &quot;markdown&quot;, padding=0L)</span>
<span class="n">```</span>
</pre></div>
</div>
</div>
</div>
<p>Cada uno de estos eventos tiene una probabilidad que se encuentra entre 0 y 1, y si sumamos la probabilidad de todos eventos, suman 1. Incluso podemos dibujar un gráfico de barras para visualizar esta distribución, como se muestra en la Figura &#64;ref(fig:pantsprob).</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-r notranslate"><div class="highlight"><pre><span></span><span class="n">```{r pantsprob, fig.cap=&quot;Demostración visual de la distribución de probabilidad de los \&quot;pantalones\&quot;. Existen 5 \&quot;eventos elementales\&quot;, que se corresponden con mis 5 pares de pantalones. Cada evento tiene una probabilidad de ocurrir: esta probabilidad es un número entre 0 y 1. La suma de estas probabilidades es 1.&quot;, echo=FALSE}</span>
<span class="n">probabilities &lt;- c( .5, .3, .1, 0, .1)</span>
<span class="n">	eventNames &lt;- c( &quot;V. azules&quot;, &quot;V. grises&quot;, &quot;V. negros&quot;, </span>
<span class="n">					 &quot;Traje negro&quot;, &quot;Chándal azul&quot; )</span>
<span class="n">	</span>
<span class="n">	# draw the plot</span>
<span class="n">	barplot( </span>
<span class="n">		height= probabilities, </span>
<span class="n">		xlab = &quot;Evento&quot;,</span>
<span class="n">		ylab = &quot;Probabilidad del evento&quot;,</span>
<span class="n">		names.arg = eventNames,</span>
<span class="n">		density = 10,</span>
<span class="n">		col = ifelse(colour,emphCol,emphGrey)</span>
<span class="n">	)</span>
<span class="n">```</span>
</pre></div>
</div>
</div>
</div>
<p>Es importante señalar que la teoría de probabilidades permite hablar acerca de eventos elementales pero también sobre los <strong><em>eventos no elementales</em></strong>. La forma más fácil de ilustrar este concepto es con un ejemplo. Siguiendo con el ejemplo de los pantalones, es perfectamente posible hablar sobre la probabilidad de usar vaqueros. Bajo esta premisa, podemos decir que el evento “yo uso vaqueros” es posible siempre y cuando ocurra alguno de los eventos elementales apropiados; en este caso “vaqueros azules”, “vaqueros negros” o “vaqueros grises”. En términos matemáticos, definimos al evento <span class="math notranslate nohighlight">\(E\)</span> “vaqueros” como el conjunto de eventos elementales <span class="math notranslate nohighlight">\((X_1, X_2, X_3)\)</span>. Si se produce alguno de estos eventos elementales, también podemos decir que <span class="math notranslate nohighlight">\(E\)</span> ha ocurrido. Por lo tanto, podemos decir que la probabilidad <span class="math notranslate nohighlight">\(P(E)\)</span> es simplemente la suma de esos tres eventos, así
$<span class="math notranslate nohighlight">\(
P(E) = P(X_1) + P(X_2) + P(X_3)
\)</span>$
y, dado que las probabilidades de los vaqueros azules, grises y negros son respectivamente .5, .3 y .1, la probabilidad total de usar vaqueros es igual a .9.</p>
<p>Todo esto parece obvio y simple. Sin embargo, a partir de estos simples comienzos, es posible construir algunas herramientas matemáticas más complejas y poderosas. En la siguiente Tabla se muestran algunas de las otras reglas que deben de cumplirse para poder calcular probabilidades.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-r notranslate"><div class="highlight"><pre><span></span><span class="n">```{r probrules, echo=FALSE}</span>
<span class="n">knitr::kable(data.frame(stringsAsFactors=FALSE,</span>
<span class="n">`Castellano` = c(&quot;No $A$&quot;, &quot;$A$ o $B$&quot;, &quot;$A$ y $B$&quot;),</span>
<span class="n">`Notacion` = c(&quot;$P(\\neg A)$&quot;, &quot;$P(A \\cup B)$&quot;, &quot;$P(A \\cap B)$&quot;),</span>
<span class="n">`Igual` = c(&quot;=&quot;, &quot;=&quot;, &quot;=&quot;),</span>
<span class="n">`Formula` = c(&quot;$1-P(A)$&quot;, &quot;$P(A) + P(B) - P(A \\cap B)$&quot;, &quot;$P(A \\vert B) P(B)$&quot;)), &quot;markdown&quot;, caption = &quot;Algunas reglas básicas de probabilidad. Son importantes si se quiere entender la teoría de la probabilidad con un poco más de detalle y nos sirven para resolver algunos problemas de probabilidad.&quot;)</span>
<span class="n">```</span>
</pre></div>
</div>
</div>
</div>
</section>
</section>
<section id="la-distribucion-binomial-binomial">
<h2>La distribución binomial{#binomial}<a class="headerlink" href="#la-distribucion-binomial-binomial" title="Permalink to this heading">#</a></h2>
<p>Como hemos visto, las distribuciones de probabilidad pueden variar enormemente, por lo que existe un gran número de distribuciones posibles. Sin embargo, no todas son igual de importantes. Las distribuciones más importantes, y de las que hablaremos en esta asignatura son cinco: la distribución binomial, la distribución normal, la distribución <span class="math notranslate nohighlight">\(t\)</span>, la distribución <span class="math notranslate nohighlight">\(\chi^2\)</span> (“chi-cuadrada”) y la distribución <span class="math notranslate nohighlight">\(F\)</span>. Daremos una breve introducción a las cinco, prestando especial atención a las distribuciones binomial y normal. Comenzaremos con la distribución más simple de las cinco, la distribución binomial.</p>
<section id="introduccion-al-binomio">
<h3>Introducción al binomio<a class="headerlink" href="#introduccion-al-binomio" title="Permalink to this heading">#</a></h3>
<p>La teoría de la probabilidad se creó originalmente para intentar describir cómo funcionaban los juegos de azar, por lo que parece adecuado que nuestra discusión sobre la <strong><em>distribución binomial</em></strong>  involucre una discusión sobre tirar dados y lanzar monedas. Imaginemos el siguiente “experimento”: tengo en mi poder 20 dados iguales de seis caras. En una de las caras de cada dado hay una imagen de una calavera; las otras cinco caras están en blanco. Si tiro los 20 dados, ¿cuál es la probabilidad de que obtenga exactamente 4 calaveras? Asumiendo que los dados son justos, sabemos que la probabilidad de que en un dado salga la calavera es de de 1 en 6; dicho de otra forma, la probabilidad de que salga calavera en un solo dado es de aproximadamente <span class="math notranslate nohighlight">\(.167\)</span>. Esta información es suficiente para poder responder a nuestra pregunta anterior, así que veamos cómo hacerlo.</p>
<p>Primero, pondremos algunos nombres a los eventos con su respectiva notación. Dejaremos que <span class="math notranslate nohighlight">\(N\)</span> denote el número de dados que se tiran en nuestro experimento; a esto se le conoce como el <strong><em>parámetro de tamaño</em></strong> de nuestra distribución binomial. También usaremos <span class="math notranslate nohighlight">\(\theta\)</span> para referirnos a la probabilidad de que al tirar un solo dado salga calavera, una cantidad que generalmente se denomina como la <strong><em>probabilidad de éxito</em></strong> del binomio.^[Hay que tener en cuenta que el término “éxito” es bastante arbitrario, y en realidad no implica que el resultado sea algo deseado. Si <span class="math notranslate nohighlight">\(\theta\)</span> se refiriera a la probabilidad de que un pasajero se lesione en un accidente de autobús, seguiría siendo una probabilidad de éxito, aunque en realidad no queremos que la gente salga lastimada] Finalmente, usaremos <span class="math notranslate nohighlight">\(X\)</span> para referirnos a los resultados de nuestro experimento, es decir, la cantidad de calaveras que obtengo cuando lanzo los dados. Dado que el valor real de <span class="math notranslate nohighlight">\(X\)</span> se debe al azar, nos podemos referir a ella como una <strong><em>variable aleatoria</em></strong>. En cualquier caso, ahora que tenemos toda esta terminología y notación, podemos utilizarlos para exponer el problema que planteábamos en el párrafo anterior con mayor precisión. La cantidad que queremos calcular es la probabilidad de que <span class="math notranslate nohighlight">\(X = 4\)</span> sabiendo que <span class="math notranslate nohighlight">\(\theta = .167\)</span> y <span class="math notranslate nohighlight">\(N=20\)</span>. La “forma” general de esta probabilidad que me interesa calcular podría escribirse como,
$<span class="math notranslate nohighlight">\(
  P(X \ | \ \theta, N)
\)</span><span class="math notranslate nohighlight">\(
donde estamos interesados en el caso específico donde \)</span>X=4<span class="math notranslate nohighlight">\(, \)</span>\theta = .167<span class="math notranslate nohighlight">\( y \)</span>N=20<span class="math notranslate nohighlight">\(. Hace falta un elemento de notación más antes de continuar con la solución del problema. Si yo quiero decir que \)</span>X<span class="math notranslate nohighlight">\( se genera aleatoriamente a partir de una distribución binomial con los parámetros \)</span>\theta<span class="math notranslate nohighlight">\( y \)</span>N<span class="math notranslate nohighlight">\(, la notación que usaría para expresarlo sería la siguiente:
  \)</span><span class="math notranslate nohighlight">\(
  X \sim \mbox{Binomial}(\theta, N)
\)</span>$</p>
<p>Aunque no utilizaremos las fórmulas para hacer cálculos formalmente, dejaré la fórmula de la distribución binomial en la Tabla &#64;ref(tab:distformulas), ya que puede ser útil si se quieren entender temas más avanzados con algo de profundidad. Por lo pronto, analizaremos como se ve una distribución binomial (puedes hacerlo tú mismo si entras <a class="reference external" href="https://leudave.shinyapps.io/distribuciones/">aquí</a>). La Figura &#64;ref(fig:binomial1) dibuja la probabilidad binomial para todos los valores posibles de <span class="math notranslate nohighlight">\(X\)</span> de nuestro experimento de lanzamiento de dados, partiendo desde <span class="math notranslate nohighlight">\(X=0\)</span>  (ninguna sale calavera) hasta <span class="math notranslate nohighlight">\(X=20\)</span> (salen todas calaveras). Hay que tener en cuenta que esto es básicamente un gráfico de barras, al igual que la gráfica de la “probabilidad de pantalones” de la Figura &#64;ref(fig:pantsprob). En el eje horizontal tenemos todos los eventos posibles, y en el eje vertical podemos leer la probabilidad de que ocurra cada uno de esos eventos. Por lo tanto, la probabilidad de que salgan 4 calaveras al tirar 20 dados es de aproximadamente 0,20 (la respuesta exacta es 0,2022036, como veremos en un momento). En otras palabras, esperaría que ese evento suceda aproximadamente el 20% de las veces que se lleve a cabo este experimento.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-r notranslate"><div class="highlight"><pre><span></span><span class="n">```{r distformulas, echo=FALSE}</span>
<span class="n">knitr::kable(data.frame(stringsAsFactors=FALSE, Binomial = c(&quot;$P(X | \\theta, N) = \\displaystyle\\frac{N!}{X! (N-X)!}  \\theta^X (1-\\theta)^{N-X}$&quot;), </span>
<span class="n">                        Normal = c(&quot;$p(X | \\mu, \\sigma) = \\displaystyle\\frac{1}{\\sqrt{2\\pi}\\sigma} \\exp \\left( -\\frac{(X - \\mu)^2}{2\\sigma^2} \\right)$ &quot;)), caption = &quot;Fórmulas para las distribuciones binomial y normal. En la ecuación de la binomial, $X!$ es una función factorial (es decir, multiplica todos los números enteros de  1 hasta $X$), y en la de la distribución normal \&quot;exp\&quot; se refiere a una función exponencial.&quot;) </span>
<span class="n">```</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-r notranslate"><div class="highlight"><pre><span></span><span class="n">```{r binomial1, fig.cap=&quot; La distribución binomial con parámetro de tamaño de $N=20$ y una probabilidad de éxito de $theta = 1/6$. Cada barra vertical representa la probabilidad de un resultado específico (un valor posible de $X$). Ya que esta es una distribución de probabilidad, cada una de las probabilidades debe ser un número entre 0 y 1, y la altura de las barras también deben sumar 1.&quot;, echo=FALSE}</span>
<span class="n"># plots the three examples of a binomial distribution</span>
<span class="n"># needed for printing</span>
<span class="n">width &lt;- 8</span>
<span class="n">height &lt;- 6</span>
<span class="n"># function to produce a styled binomial plot</span>
<span class="n">binomPlot &lt;- function( n,p, ... ) {</span>
<span class="n">  </span>
<span class="n">  # probabilities of each outcome</span>
<span class="n">  out &lt;- 0:n</span>
<span class="n">  prob &lt;- dbinom( x=out, size=n, prob=p )</span>
<span class="n">  </span>
<span class="n">  # plot</span>
<span class="n">  plot( </span>
<span class="n">    out, prob, type=&quot;h&quot;, lwd=3, ylab=&quot;Probabilidad&quot;, </span>
<span class="n">    frame.plot=FALSE, col=ifelse(colour,emphCol,&quot;black&quot;), ...</span>
<span class="n">  )</span>
<span class="n">  </span>
<span class="n">}</span>
<span class="n"># skulls image...</span>
<span class="n">binomPlot( n=20, p=1/6, xlab=&quot;Número de calaveras observadas&quot; )</span>
<span class="n">```</span>
</pre></div>
</div>
</div>
</div>
<p>Para darte una idea de cómo cambia la distribución binomial cuando modificamos los valores de <span class="math notranslate nohighlight">\(\theta\)</span> y <span class="math notranslate nohighlight">\(N\)</span>, supongamos que en lugar de tirar dados, en realidad estoy lanzando monedas. Esta vez, mi experimento implica lanzar una moneda justa repetidamente, y el resultado que me interesa es la cantidad de caras que observo. En este escenario, la probabilidad de éxito ahora es de <span class="math notranslate nohighlight">\(\theta = 1/2\)</span>. Supongamos que tirara la moneda <span class="math notranslate nohighlight">\(N=20\)</span>  veces. En este ejemplo, he cambiado la probabilidad de éxito, pero mantuve el tamaño de la muestra del experimento. ¿Qué efecto tiene este cambio en nuestra distribución binomial? Bueno, como la Figura &#64;ref(fig:binomial2a)  muestra, el efecto principal fue el desplazamiento de toda la distribución hacia la derecha, como era de esperar. ¿Y si lanzamos una moneda <span class="math notranslate nohighlight">\(N=100\)</span> veces? En este caso obtendremos algo como lo de la Figura &#64;ref(fig:binomial2b). La distribución se mantiene aproximadamente en el medio, pero hay un poco más de variabilidad en los posibles resultados.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-r notranslate"><div class="highlight"><pre><span></span><span class="n">```{r binomial2a, fig.cap=&quot;Dos distribuciones binomiales, que involucran un escenario en el que lanzo una moneda justa, donde la probabilidad de éxito es $theta = 1/2$. Asumimos que estoy lanzando la moneda $N=20$ veces.&quot;, echo=FALSE}</span>
<span class="n">  # plots the three examples of a binomial distribution</span>
<span class="n">  	# needed for printing</span>
<span class="n">  	width &lt;- 8</span>
<span class="n">  	height &lt;- 6</span>
<span class="n">  	# function to produce a styled binomial plot</span>
<span class="n">  	binomPlot &lt;- function( n,p, ... ) {</span>
<span class="n">  		# probabilities of each outcome</span>
<span class="n">  		out &lt;- 0:n</span>
<span class="n">  		prob &lt;- dbinom( x=out, size=n, prob=p )</span>
<span class="n">  		# plot</span>
<span class="n">  		plot(</span>
<span class="n">  			out, prob, type=&quot;h&quot;, lwd=3, ylab=&quot;Probabilidad&quot;,</span>
<span class="n">  			frame.plot=FALSE, col=ifelse(colour,emphCol,&quot;black&quot;), ...</span>
<span class="n">  		)</span>
<span class="n">  	}</span>
<span class="n">  	# coins image #1...</span>
<span class="n">  	binomPlot( n=20, p=1/2, xlab=&quot;Número de caras observadas&quot; )</span>
<span class="n">```</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-r notranslate"><div class="highlight"><pre><span></span><span class="n">```{r binomial2b, fig.cap=&quot;Dos distribuciones binomiales, que involucran un escenario en el lanzo una moneda justa, donde la probabilidad de éxito subyacente es $theta = 1/2$. Asumimos que estoy lanzando la moneda $N=100$ veces.&quot;, echo=FALSE}</span>
<span class="n">  # plots the three examples of a binomial distribution</span>
<span class="n">  	# needed for printing</span>
<span class="n">  	width &lt;- 8</span>
<span class="n">  	height &lt;- 6</span>
<span class="n">  	# function to produce a styled binomial plot</span>
<span class="n">  	binomPlot &lt;- function( n,p, ... ) {</span>
<span class="n">  		# probabilities of each outcome</span>
<span class="n">  		out &lt;- 0:n</span>
<span class="n">  		prob &lt;- dbinom( x=out, size=n, prob=p )</span>
<span class="n">  		# plot</span>
<span class="n">  		plot(</span>
<span class="n">  			out, prob, type=&quot;h&quot;, lwd=3, ylab=&quot;Probabilidad&quot;,</span>
<span class="n">  			frame.plot=FALSE, col=ifelse(colour,emphCol,&quot;black&quot;), ...</span>
<span class="n">  		)</span>
<span class="n">  	}</span>
<span class="n">  	# coins image #2...</span>
<span class="n">  	binomPlot( n=100, p=1/2, xlab=&quot;Número de caras observadas&quot; )</span>
<span class="n">```</span>
</pre></div>
</div>
</div>
</div>
</section>
</section>
<section id="la-distribucion-normal-normal">
<h2>La distribución normal{#normal}<a class="headerlink" href="#la-distribucion-normal-normal" title="Permalink to this heading">#</a></h2>
<p>Si bien la distribución binomial es conceptualmente la distribución más sencilla de entender, no es la más importante. Ese honor le corresponde a la <strong><em>distribución normal</em></strong>, también conocida como “curva de campana” o como “distribución gaussiana” o “campana de Gauss”. Una distribución normal se describe utilizando dos parámetros, la media de la distribución <span class="math notranslate nohighlight">\(\mu\)</span>  y la desviación estándar de la distribución <span class="math notranslate nohighlight">\(\sigma\)</span>. La notación que utilizamos para decir que una variable <span class="math notranslate nohighlight">\(X\)</span> se distribuye normalmente es la siguiente:</p>
<div class="math notranslate nohighlight">
\[
  X \sim \mbox{Normal}(\mu,\sigma)
\]</div>
<p>Al igual que con la distribución binomial, he incluido la fórmula para la distribución normal en la tabla &#64;ref(tab:distformulas), porque creo que es lo suficientemente importante como para que todos los que aprenden algo de estadística al menos la conozcan, aunque no nos enfoquemos en ella.</p>
<p>Vamos intentar descifrar lo que significa que una variable esté normalmente distribuida. Echemos un vistazo a la Figura &#64;ref(fig:normdist), que muestra una distribución normal con media <span class="math notranslate nohighlight">\(\mu = 0\)</span> y desviación estándar <span class="math notranslate nohighlight">\(\sigma = 1\)</span>. Con un poco de imaginación, podemos apreciar de dónde viene el nombre “curva de campana”. A diferencia de los gráficos sobre la distribución binomial, la imagen de la distribución normal en la Figura &#64;ref(fig:normdist) muestra una curva suave en lugar de barras “tipo histograma”. Esto no es arbitrario: la distribución normal es continua, mientras que la distribución binomial es discreta. Por ejemplo, en el experimento de tiro de dados de la sección anterior, es posible obtener 3 calaveras o 4 calaveras, mientras que un valor intermedio como 3.9 es imposible de obtener. Este hecho se ve reflejado en la Figura &#64;ref(fig:binomial1), donde tenemos una barra ubicada en <span class="math notranslate nohighlight">\(X=3\)</span> y otra en <span class="math notranslate nohighlight">\(X=4\)</span>, pero entre ellas no hay nada. En cambio, los valores continuos no tienen esta restricción. Por ejemplo, supongamos que estamos hablando del tiempo. La temperatura de un día primavera podría ser de 23 grados, 24 grados, 23.9 grados o cualquier cosa intermedia, ya que la temperatura es una variable continua, por lo que una distribución normal podría ser la herramienta apropiada para describir las diferentes temperaturas en los días de primavera.^[En la práctica, la distribución normal es tan útil que las personas tienden a usarla incluso cuando la variable no es continua. Siempre que haya suficientes categorías (por ejemplo, respuestas de escala Likert de un cuestionario), suele ser frecuente el uso de la distribución normal.]</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-r notranslate"><div class="highlight"><pre><span></span><span class="n">```{r normdist, fig.cap=&quot; Distribución normal con media $mu = 0$ y desviación estándar $sigma = 1$. El eje $x$ corresponde con el valor de alguna variable, y el eje $y$ nos dice qué tan probable es que observemos ese valor. Sin embargo, vemos como el eje $y$ se denomina \&quot;Densidad de Probabilidad\&quot; y no \&quot;Probabilidad\&quot;. La altura de la curva no representa como tal la probabilidad de observar un valor particular de $x$. Sin embargo, las alturas nos informan sobre qué valores de $x$ son más probables (¡los más altos!).&quot;, echo=FALSE}</span>
<span class="n"># plots the standard normal</span>
<span class="n"># needed for printing</span>
<span class="n">width &lt;- 8</span>
<span class="n">height &lt;- 6</span>
<span class="n">fileName &lt;- &quot;standardNormal.eps&quot;	</span>
<span class="n"># draw the plot</span>
<span class="n">xval &lt;- seq(-3,3,.01)</span>
<span class="n">yval &lt;- dnorm( xval, 0, 1)</span>
<span class="n">plot( 	xval, yval, lwd=3, ylab=&quot;Densidad de probabilidad&quot;, xlab=&quot;Valor observado&quot;,</span>
<span class="n">       frame.plot = FALSE, col=ifelse(colour,emphCol,&quot;black&quot;), type=&quot;l&quot;</span>
<span class="n">)</span>
<span class="n">```</span>
</pre></div>
</div>
</div>
</div>
<p>Una vez visto esto, vamos a analizar cómo funciona una distribución normal. En primer lugar, veamos qué es lo que sucede cuando jugamos con los parámetros de la distribución (puedes hacerlo tú mismo si entras en este enlace). La Figura &#64;ref(fig:normmean) muestra distribuciones normales que tienen medias diferentes, pero con la misma desviación estándar. Como es de esperar, todas estas distribuciones tienen la misma “anchura”. La unica diferencia entre ellas es que se han desplazado hacia la izquierda o hacia la derecha. En todos los demás aspectos son idénticas. Por el contrario, si aumentamos la desviación estándar mientras mantenemos la media constante, el pico de la distribución permanece en el mismo lugar, pero la distribución se amplía, como podemos ver en la Figura &#64;ref(fig:normsd). Sin embargo, cuando ampliamos la distribución, la altura del pico disminuye. Esto <em>tiene</em> que suceder: de la misma forma que las alturas de las barras de una distribución binomial discreta tienen que <em>sumar</em> 1, el total del <em>área bajo la curva</em> de una distribución normal debe ser igual a 1.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-r notranslate"><div class="highlight"><pre><span></span><span class="n">```{r normmean, fig.cap=&quot;Gráfica que demuestra lo que sucede cuando se cambia la media de una distribución normal. La línea sólida representa una distribución normal con media de $mu=4$.  La línea discontinua muestra una distribución normal con una media de $mu=7$.  En ambos casos, la desviación estándar es de $sigma=1$. Vemos como las dos distribuciones tienen la misma forma, pero la distribución con la línea discontinua se desplaza hacia la derecha.&quot;, echo=FALSE}</span>
<span class="n"># draw the plot</span>
<span class="n">xval &lt;- seq(0,11,.01)</span>
<span class="n">yval.1 &lt;- dnorm( xval, 4, 1)</span>
<span class="n">yval.2 &lt;- dnorm( xval, 7, 1)</span>
<span class="n">plot( 	xval, yval.1, lwd=3, ylab=&quot;Densidad de probabilidad&quot;, xlab=&quot;Valor observado&quot;,</span>
<span class="n">       frame.plot = FALSE, col=ifelse(colour,emphCol,&quot;black&quot;), type=&quot;l&quot;</span>
<span class="n">)</span>
<span class="n">lines(	xval, yval.2, lwd=3, col=ifelse(colour,emphCol,&quot;black&quot;), lty=2 )</span>
<span class="n">```</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-r notranslate"><div class="highlight"><pre><span></span><span class="n">```{r normsd, fig.cap=&quot;Una ilustración de lo que sucede cuando cambia la desviación estándar de una distribución normal. Ambas distribuciones tienen una media de $mu=5$, pero diferentes desviaciones estándar. La línea continua dibuja una distribución con una desviación estándar $sigma=1$, y la línea discontinua muestra una distribución con desviación estándar de $sigma=2$. Como consecuencia, ambas distribuciones están centradas en el mismo lugar, pero la distribución con la línea discontinua es más ancha que la otra.&quot;, echo=FALSE}</span>
<span class="n"># draw the plot</span>
<span class="n">xval &lt;- seq(0,10,.01)</span>
<span class="n">yval.1 &lt;- dnorm( xval, 5, 1)</span>
<span class="n">yval.2 &lt;- dnorm( xval, 5, 2)</span>
<span class="n">plot( 	xval, yval.1, lwd=3, ylab=&quot;Densidad de probabilidad&quot;, xlab=&quot;Valor observado&quot;,</span>
<span class="n">       frame.plot = FALSE, col=ifelse(colour,emphCol,&quot;black&quot;), type=&quot;l&quot;</span>
<span class="n">)</span>
<span class="n">lines(	xval, yval.2, lwd=3, col=ifelse(colour,emphCol,&quot;black&quot;), lty=2 )</span>
<span class="n">```</span>
</pre></div>
</div>
</div>
</div>
<p>Antes de seguir adelante, quiero señalar una característica importante de la distribución normal. Independientemente de los valores de la media y la desviación estándar, un 68.3% del área de la curva cae dentro de 1 desviación estándar sobre la media. Del mismo modo, el 95.4% de la distribución cae dentro de 2 desviaciones estándar sobre la media, y el 99.7% de la distribución está dentro de 3 desviaciones estándar. Esta idea se ilustra en las Figuras &#64;ref(fig:sdnorm1) y &#64;ref(fig:sdnorm2).</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-r notranslate"><div class="highlight"><pre><span></span><span class="n">```{r sdnorm1, fig.cap=&quot;El área bajo la curva indica la probabilidad de que una observación se encuentre dentro de un rango determinado. Las línea continua traza una distribución normal con media $mu=0$ y desviación estándar $sigma=1$. El área sombreada ilustra el \&#39;área bajo la curva&#39;\ para dos casos importantes. En el panel a, podemos ver que hay es un  68.3% de probabilidad de que una observación caiga dentro de 1 desviación estándar sobre la media. En el panel b, vemos que existe una probabilidad del 95.4% de que una observación se encuentre dentro de 2 desviaciones estándar sobre la media.&quot;, echo=FALSE}</span>
<span class="n">par(mfrow=c(1,2))</span>
<span class="n">plotOne &lt;- function( a,b ) {</span>
<span class="n">  plot.new()</span>
<span class="n">  w&lt;-4</span>
<span class="n">  plot.window( xlim = c(-w,w), ylim = c(0,.4))</span>
<span class="n">  xval &lt;- seq( max(a,-w),min(b,w),.01)</span>
<span class="n">  yval &lt;- dnorm(xval,0,1)</span>
<span class="n">  end &lt;- length(xval)</span>
<span class="n">  polygon( c(xval[1],xval,xval[end]), </span>
<span class="n">           c(0,yval,0),</span>
<span class="n">           col=ifelse(colour,emphCol,&quot;black&quot;),</span>
<span class="n">           density = 10 </span>
<span class="n">  )</span>
<span class="n">  xval &lt;- seq(-w,w,.01)</span>
<span class="n">  yval &lt;- dnorm( xval, 0, 1)				</span>
<span class="n">  lines( xval,yval, lwd=2, col=&quot;black&quot; )</span>
<span class="n">  axis( side=1, at=-w:w )</span>
<span class="n">  area &lt;- abs(pnorm(b,0,1)-pnorm(a,0,1))</span>
<span class="n">  title( main= paste(&quot;Área bajo la curva = &quot;,round(area*100,1),&quot;%&quot;, sep=&quot;&quot;), font.main=1 )</span>
<span class="n">  </span>
<span class="n">}</span>
<span class="n">plotOne(-1,1)</span>
<span class="n">plotOne(-3,3)</span>
<span class="n">par(mfrow=c(1,1))</span>
<span class="n">```</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-r notranslate"><div class="highlight"><pre><span></span><span class="n">```{r sdnorm2, fig.cap=&quot;Dos ejemplos más sobre el concepto del &#39;área bajo la curva&#39;. Existe un 15.9% de probabilidad de que una observación se encuentre 1 desviación estándar o menos por debajo de la media (panel a), y una probabilidad del 34.1% de que una observación sea mayor que una desviación estándar por debajo de la media pero menor que la media (panel b). Si sumamos estos dos valores, obtendremos 15.9% + 34.1% = 50%. Para datos que estén normalmente distribuidos, existe un 50% de probabilidad de que una observación caiga por debajo de la media. Esto implica que existe un 50% de probabilidad de que caiga por encima de la media.&quot;, echo=FALSE}</span>
<span class="n">par(mfrow=c(1,2))</span>
<span class="n">plotOne(-1,0)</span>
<span class="n">plotOne(-50,-1)</span>
<span class="n">par(mfrow=c(1,1))</span>
<span class="n">```</span>
</pre></div>
</div>
</div>
</div>
<section id="densidad-de-probabilidad-density">
<h3>Densidad de probabilidad{#density}<a class="headerlink" href="#densidad-de-probabilidad-density" title="Permalink to this heading">#</a></h3>
<p>A lo largo de la discusión sobre la distribución normal, ha habido un par de cosas que parecen no tener sentido. Quizás hayas notado que el eje <span class="math notranslate nohighlight">\(y\)</span> en estas Figuras se denomina como “Densidad de probabilidad” en lugar de “Probabilidad”. Tal vez notaste que utilizamos <span class="math notranslate nohighlight">\(p(X)\)</span> en lugar de <span class="math notranslate nohighlight">\(P(X)\)</span>  en la fórmula de la distribución normal.</p>
<p>Si utilizamos la Figura y calculamos (siguiendo la fórmula) la probabilidad de <code class="docutils literal notranslate"><span class="pre">x</span> <span class="pre">=</span> <span class="pre">1</span></code>,  para una variable normalmente distribuida con <code class="docutils literal notranslate"><span class="pre">media</span> <span class="pre">=</span> <span class="pre">1</span></code> y desviación estándar <code class="docutils literal notranslate"><span class="pre">sd</span> <span class="pre">=</span> <span class="pre">0.1</span></code>, nos arrojará como resultado una probabilidad de 3.99. Sin embargo, hemos visto anteriormente que las probabilidades <em>no</em> pueden ser mayores que 1. Entonces, ¿qué es lo que hemos calculado?</p>
<p>Lo que hemos calculado aquí en realidad no es una probabilidad: Para entender qué es ese algo, tenemos que pensar qué es lo que realmente <em>significa</em> decir que <span class="math notranslate nohighlight">\(X\)</span> es una variable continua. Digamos que estamos hablando de la temperatura otra vez. El termómetro me dice que hacen 23 grados, pero yo sé que eso no es del todo cierto. No hacen 23 grados <em>exactamente</em>. Quizás sea algo más cercano a los 23.1 grados o, si seguimos, en realidad podrían ser 23.095 grados. Esto es lo que sucede con los valores continuos: nunca se sabe el valor exacto.</p>
<p>Ahora pensemos en lo que esto implica cuando hablamos de probabilidades. Supongamos la temperatura máxima para mañana se toma de una distribución normal con media 23 y desviación estándar 1. ¿Cuál es la probabilidad de que la temperatura sea <em>exactamente</em> 23 grados? La respuesta es “cero”, o posiblemente, “un número tan cercano a cero que bien podría ser cero”. ¿Por qué es esto? Es como intentar tirar un dardo en un tablero de dardos con dianas infinitamente cada vez más pequeñas: no importa cuán buena sea tu puntería, nunca acertarás. En la vida real nunca obtendremos el valor exacto de 23. Siempre será 23.1 o 22.99998 o algo así. En en otras palabras, no tiene sentido hablar de la probabilidad de que la temperatura sea exactamente 23 grados. Sin embargo, en el día a día, si el termómetro indica 23 grados pero en realidad hacen 22.9998 grados, probablemente no nos importe demasiado. Esto es porque en el día a día, “23 grados” por lo general significa algo así como “en algún lugar entre 22.5 y 23.5 grados”. Y aunque no parezca muy importante preguntar por la probabilidad de que la temperatura sea exactamente 23 grados, lo que sí lo parece es preguntar sobre la probabilidad de que la temperatura se encuentre entre 22.5 y 23.5, o entre 20 y 30, o cualquier otro rango de temperaturas en el que estemos interesados.</p>
<p>El objetivo de esta explicación es dejar claro que, cuando hablamos de distribuciones continuas, no tiene sentido hablar sobre la probabilidad de un valor específico. Sin embargo, sí que <em>podemos</em> hablar sobre la probabilidad de que el valor se encuentre dentro de un rango particular de valores. Para encontrar probabilidad asociada con un rango particular, lo que debe hacer es calcular el “área bajo la curva”. Este concepto lo conocemos: en la Figura &#64;ref(fig:sdnorm1), las áreas sombreadas representan probabilidades genuinas (por ejemplo, la Figura &#64;ref(fig:sdnorm1) muestra la probabilidad de observar un valor que cae dentro de 1 desviación estándar sobre la media).</p>
<p>Para finalizar, volveremos con la fórmula para <span class="math notranslate nohighlight">\(p(x)\)</span> que vimos anteriormente. Los resultados de <span class="math notranslate nohighlight">\(p(x)\)</span> no describe una probabilidad, sino una <strong><em>densidad de probabilidad</em></strong>,  que en las gráficas corresponde a la altura de la curva. De la misma forma en que las probabilidades son números no-negativos que deben sumar 1, las densidades de probabilidad son números no-negativos que deben integrar a 1 (donde la integral se toma a través de todos los valores posibles de <span class="math notranslate nohighlight">\(X\)</span>). Para calcular la probabilidad de que <span class="math notranslate nohighlight">\(X\)</span> caiga entre <span class="math notranslate nohighlight">\(a\)</span> y <span class="math notranslate nohighlight">\(b\)</span>  calculamos la integral definida de la función de densidad sobre el rango correspondiente, <span class="math notranslate nohighlight">\(\int_a^b p(x) \ dx\)</span>. Se trata simplemente de otra forma de llegar al mismo resultado.</p>
</section>
</section>
<section id="otras-distribuciones-utiles-otherdists">
<h2>Otras distribuciones útiles{#otherdists}<a class="headerlink" href="#otras-distribuciones-utiles-otherdists" title="Permalink to this heading">#</a></h2>
<p>La distribución normal es la distribución más utilizada por los estadísticos (por razones que se discutirán más adelante), y la distribución binomial es útil muchos escenarios. Sin embargo, existen otros tipos de distribuciones de probabilidad. Revisaremos brevemente 3 de ellas: la distribución <span class="math notranslate nohighlight">\(t\)</span>, la distribución <span class="math notranslate nohighlight">\(\chi^2\)</span> y la distribución <span class="math notranslate nohighlight">\(F\)</span>.  La <strong><em>distribución <span class="math notranslate nohighlight">\(t\)</span></em></strong> es una distribución continua que se parece mucho a una distribución normal, pero que tiene colas más pesadas (ver Figura &#64;ref(fig:tdist)). Esta distribución tiende a surgir en situaciones en las que piensa que los datos siguen una distribución normal, pero no se conoce la media o la desviación estándar.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-r notranslate"><div class="highlight"><pre><span></span><span class="n">```{r tdist, fig.cap=&quot;Una distribución $t$ con 3 grados de libertad (línea continua). Se asemeja a una distribución normal, pero no es igual (línea discontinua). Ten en cuenta que las \&quot;colas\&quot; de la distribución $t$ son más \&quot;pesadas\&quot;  (es decir, se extienden más hacia afuera, conteniendo más valores que se alejan de la media) que las colas de la distribución normal.&quot;, echo=FALSE}</span>
<span class="n">xval &lt;- seq(-5,5,.01)</span>
<span class="n">yval &lt;- dt( xval, df=3)</span>
<span class="n">plot( 	xval, yval, lwd=3, ylab=&quot;Densidad de probabilidad&quot;, xlab=&quot;Valor observado&quot;,</span>
<span class="n">       frame.plot = FALSE, col=ifelse(colour,emphCol,&quot;black&quot;), type=&quot;l&quot;, ylim=c(0,.4)</span>
<span class="n">)</span>
<span class="n">lines( xval, dnorm(xval,0,1), lty=2, col=emphGrey)</span>
<span class="n">```</span>
</pre></div>
</div>
</div>
</div>
<ul class="simple">
<li><p>La <strong><em>distribución <span class="math notranslate nohighlight">\(\chi^2\)</span></em></strong> es otra distribución que podemos encontrar con cierta frecuencia. Es habitual encontrarla cuando hacemos análisis de datos categóricos. Los valores de una distribución <span class="math notranslate nohighlight">\(\chi^2\)</span> se consiguen al elevar al cuadrado los valores de una variable distribuída normalmente y luego sumarlos (un procedimiento denominado “suma de cuadrados”). Después veremos porqué es útil hacer una “suma de cuadrados”. La apariencia de una distribución <span class="math notranslate nohighlight">\(\chi^2\)</span> la puedes encontrar en la Figura &#64;ref(fig:chisqdist).</p></li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-r notranslate"><div class="highlight"><pre><span></span><span class="n">```{r chisqdist, fig.cap=&quot;Una distribución $chi^2$ con 3 grados de libertad (3 repeticiones, lo explicaremos más adelante). Observa que los valores siempre deben ser mayores que cero (los valores se elevan al cuadrado y se suman), y que la distribución es bastante sesgada (en este caso hacia la derecha). Estas son las características clave de una distribución chi-cuadrado.&quot;, echo=FALSE}</span>
<span class="n">xval &lt;- seq(0,10,.01)</span>
<span class="n">yval &lt;- dchisq( xval, df=3)</span>
<span class="n">plot( 	xval, yval, lwd=3, ylab=&quot;Densidad de probabilidad&quot;, xlab=&quot;Valor observado&quot;,</span>
<span class="n">       frame.plot = FALSE, col=ifelse(colour,emphCol,&quot;black&quot;), type=&quot;l&quot;</span>
<span class="n">)</span>
<span class="n">```</span>
</pre></div>
</div>
</div>
</div>
<ul class="simple">
<li><p>La <strong><em>distribución <span class="math notranslate nohighlight">\(F\)</span></em></strong> se parece un poco a la distribución <span class="math notranslate nohighlight">\(\chi^2\)</span> y surge cada vez que necesitamos comparar dos distribuciones <span class="math notranslate nohighlight">\(\chi^2\)</span> entre sí. Es decir, si queremos comparar dos “sumas de cuadrados” diferentes, nos encontraremos con una distribución <span class="math notranslate nohighlight">\(F\)</span>. Aún no hemos visto un ejemplo de todo lo que implica una suma de cuadrados, pero lo veremos cuando hablemos sobre ANOVAs, donde nos encontraremos nuevamente con la distribución <span class="math notranslate nohighlight">\(F\)</span>.</p></li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-r notranslate"><div class="highlight"><pre><span></span><span class="n">```{r Fdist, fig.cap=&quot;Una distribución $F$ con 3 y 5 grados de libertad. Cualitativamente hablando, es similar a una distribución de chi-cuadrado, pero por lo general el significado no es el mismo.&quot;, echo=FALSE}</span>
<span class="n"># draw the plot</span>
<span class="n">xval &lt;- seq(0,10,.01)</span>
<span class="n">yval &lt;- df( xval, df1=3, df2=5)</span>
<span class="n">plot( 	xval, yval, lwd=3, ylab=&quot;Densidad de probabilidad&quot;, xlab=&quot;Valor observado&quot;,</span>
<span class="n">       frame.plot = FALSE, col=ifelse(colour,emphCol,&quot;black&quot;), type=&quot;l&quot;</span>
<span class="n">)</span>
<span class="n">```</span>
</pre></div>
</div>
</div>
</div>
<p>Debido a que estas distribuciones están estrechamente relacionadas con la distribución normal y entre sí, y porque se convertirán en las distribuciones importantes al hacer análisis estadísticos inferenciales en este curso, creo que es útil hacer una pequeña demostración de cómo estas distribuciones realmente están relacionadas entre sí. Primero, imagina que tenemos un conjunto de 1,000 observaciones aleatorias distribuidas normalmente al cual llamaremos “Muestra A”.</p>
<p>Esta “Muestra A” es una variable que contiene 1,000 números que se distribuyen normalmente y tienen una media de 0 y desviación estándar de 1. En la Figura &#64;ref(fig:distnormal) podemos ver un histograma con la distribución de los valores organizados por columnas, así como una línea negra sólida que representa la distribución verdadera de los datos (es decir, una distribución normal con valores infinitos con media 0 y desviación estándar 1). Así podemos comparar los datos recién generados con los de una distribución normal verdadera.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-r notranslate"><div class="highlight"><pre><span></span><span class="n">```{r distnormal, echo=FALSE, fig.cap=&quot;Distribución normal de la Muestra A (histograma), junto con la distribución normal verdadera (línea sólida)&quot;}</span>
<span class="n"># generate the data </span>
<span class="n">n &lt;- 1000</span>
<span class="n">normal.a &lt;- rnorm( n )</span>
<span class="n">normal.b &lt;- rnorm( n )</span>
<span class="n">normal.c &lt;- rnorm( n )</span>
<span class="n">chi.sq.3 &lt;- (normal.a)^2 + (normal.b)^2 + (normal.c)^2	</span>
<span class="n">normal.d &lt;- rnorm( n )</span>
<span class="n">t.3 &lt;- normal.d / sqrt( chi.sq.3 / 3 )</span>
<span class="n">chi.sq.20 &lt;- rchisq( n, 20)</span>
<span class="n">F.3.20 &lt;- (chi.sq.3 / 3) / (chi.sq.20 / 20)</span>
<span class="n"># histogram for the normal data</span>
<span class="n">bw &lt;- .25</span>
<span class="n">hist( 	normal.a, seq(min(normal.a)-bw,max(normal.a)+bw,bw),</span>
<span class="n">       freq=FALSE, xlim=c(-4,4), </span>
<span class="n">       col=ifelse(colour,emphColLight,emphGrey),</span>
<span class="n">       border=&quot;white&quot;, ylim=c(0,.45), axes=FALSE,</span>
<span class="n">       xlab=&quot;&quot;,ylab=&quot;&quot;, main=&quot;Distribución normal&quot;,</span>
<span class="n">       font.main = 1</span>
<span class="n">)</span>
<span class="n">lines( x&lt;-seq(-4,4,.1), dnorm(x), lwd=3, col=&quot;black&quot;  )</span>
<span class="n">axis(1)</span>
<span class="n">```</span>
</pre></div>
</div>
</div>
</div>
<p>En la Figura anterior podemos observar cómo han sido generados muchos valores distribuidos normalmente que luego han sido comparados con la  distribución de probabilidad verdadera (línea sólida). Supongamos que ahora queremos una distribución chi-cuadrada con 3 grados de libertad. Como hemos mencionado anteriormente, una distribución chi-cuadrada con <span class="math notranslate nohighlight">\(k\)</span> grados de libertad es es el resultado de tomar <span class="math notranslate nohighlight">\(k\)</span> variables (o muestras) normalmente distribuidas (con media 0 y desviación estándar 1), elevarlas al cuadrado y sumarlas. Como queremos una distribución de chi-cuadrada con 3 grados de libertad, además de nuestra “Muestra A”, necesitamos dos conjuntos más de valores (también distribuidos normalmente). A estas nuevas dos variables las llamaremos “Muestra B” y “Muestra C”:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-r notranslate"><div class="highlight"><pre><span></span><span class="n">```{r distchi, echo=FALSE, fig.cap=&quot;Distribución chi-cuadrada. Incluye a las Muestras A, B y C (3 grados de libertad)&quot;}</span>
<span class="n"># generate the data </span>
<span class="n">n &lt;- 1000</span>
<span class="n">normal.a &lt;- rnorm( n )</span>
<span class="n">normal.b &lt;- rnorm( n )</span>
<span class="n">normal.c &lt;- rnorm( n )</span>
<span class="n">chi.sq.3 &lt;- (normal.a)^2 + (normal.b)^2 + (normal.c)^2	</span>
<span class="n">normal.d &lt;- rnorm( n )</span>
<span class="n">t.3 &lt;- normal.d / sqrt( chi.sq.3 / 3 )</span>
<span class="n">chi.sq.20 &lt;- rchisq( n, 20)</span>
<span class="n">F.3.20 &lt;- (chi.sq.3 / 3) / (chi.sq.20 / 20)</span>
<span class="n"># histogram for the chisquared</span>
<span class="n">bw&lt;- .5</span>
<span class="n">hist( 	chi.sq.3, seq(0,max(chi.sq.3)+bw,bw),</span>
<span class="n">       freq=FALSE, xlim=c(0,16), </span>
<span class="n">       col=ifelse(colour,emphColLight,emphGrey),</span>
<span class="n">       border=&quot;white&quot;, axes=FALSE, ylim=c(0,.25),</span>
<span class="n">       xlab=&quot;&quot;,ylab=&quot;&quot;, main=&quot;Distribución chi-cuadrada&quot;,</span>
<span class="n">       font.main=1</span>
<span class="n">)</span>
<span class="n">lines( x&lt;-seq(0,16,.1), dchisq(x,3), lwd=3, col=&quot;black&quot;  )	</span>
<span class="n">axis(1)</span>
<span class="n">```</span>
</pre></div>
</div>
</div>
</div>
<p>Una vez que tenemos las tres variables, la teoría dice que debemos elevarlos al cuadrado y sumarlos, con lo que obtendremos 1,000 observaciones que siguen una distribución de chi-cuadrada con 3 grados de libertad. Visualmente, obtendremos una distribución como en la Figura &#64;ref(fig:distchi).</p>
<p>Podemos extender esta demostración y tratar de entender el origen de la distribución <span class="math notranslate nohighlight">\(t\)</span> y la distribución <span class="math notranslate nohighlight">\(F\)</span>. Antes, hemos dicho que la distribución <span class="math notranslate nohighlight">\(t\)</span> está relacionada con la distribución normal cuando se desconoce la media o la desviación estándar. Sin embargo, existe una relación más precisa entre las distribuciones normal, chi-cuadrada y <span class="math notranslate nohighlight">\(t\)</span>. Supongamos que “escalamos” nuestros datos anteriores de la chi-cuadrada al dividirla entre sus 3 grados de libertad.</p>
<p>Si tomamos un conjunto de variables normalmente distribuidas (pensemos ahora en una “Muestra D”) y las dividimos por (la raíz cuadrada de) nuestra variable chi-cuadrada “escalada” que tenía <span class="math notranslate nohighlight">\(k=3\)</span> grados de libertad, la operación dará como resultado una distribución <span class="math notranslate nohighlight">\(t\)</span> con 3 grados de libertad. Si trazamos el histograma de esta nueva distribución <span class="math notranslate nohighlight">\(t\)</span>, observaremos algo parecido al de la Figura &#64;ref(fig:distt).</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-r notranslate"><div class="highlight"><pre><span></span><span class="n">```{r distt, echo=FALSE, fig.cap=&quot;Distribución t. Es el resultado de dividir una distribución normal (en este caso la Muestra D) entre una variable chi-cuadrada escalada&quot;}</span>
<span class="n"># generate the data </span>
<span class="n">n &lt;- 1000</span>
<span class="n">normal.a &lt;- rnorm( n )</span>
<span class="n">normal.b &lt;- rnorm( n )</span>
<span class="n">normal.c &lt;- rnorm( n )</span>
<span class="n">chi.sq.3 &lt;- (normal.a)^2 + (normal.b)^2 + (normal.c)^2	</span>
<span class="n">normal.d &lt;- rnorm( n )</span>
<span class="n">t.3 &lt;- normal.d / sqrt( chi.sq.3 / 3 )</span>
<span class="n">chi.sq.20 &lt;- rchisq( n, 20)</span>
<span class="n">F.3.20 &lt;- (chi.sq.3 / 3) / (chi.sq.20 / 20)</span>
<span class="n"># histogram for the t data</span>
<span class="n">bw &lt;- .3</span>
<span class="n">hist( 	t.3, seq(min(t.3)-bw,max(t.3)+bw,bw),</span>
<span class="n">       freq=FALSE, xlim=c(-5,5), </span>
<span class="n">       col=ifelse(colour,emphColLight,emphGrey),</span>
<span class="n">       border=&quot;white&quot;, axes=FALSE, ylim=c(0,.4),</span>
<span class="n">       xlab=&quot;&quot;,ylab=&quot;&quot;, main=&quot;Distribución t&quot;,</span>
<span class="n">       font.main = 1</span>
<span class="n">)</span>
<span class="n">lines( x&lt;-seq(-4,4,.1), dt(x,3), lwd=3, col=&quot;black&quot;  )</span>
<span class="n">axis(1)	</span>
<span class="n">```</span>
</pre></div>
</div>
</div>
</div>
<p>Del mismo modo, podemos obtener una distribución <span class="math notranslate nohighlight">\(F\)</span> al dividir dos distribuciones chi-cuadrada “escaladas”. Supongamos, por ejemplo, que deseamos generar datos que sigan una distribución <span class="math notranslate nohighlight">\(F\)</span> con 3 y 20 grados de libertad (es decir, con 3 y 20 variables respectivamente). La división de los valores de ambas distribuciones nos da como resultado una nueva variable <code class="docutils literal notranslate"><span class="pre">F.3.20</span></code> y su distribución es la que se muestra en la Figura &#64;ref(fig:distf).</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-r notranslate"><div class="highlight"><pre><span></span><span class="n">```{r distf, echo=FALSE, fig.cap=&quot;Distribución F. En este ejemplo hipotético, se compara la distribución chi-cuadrada de 3 grados de libertad previa con otra distribución chi-cuadrada con 20 grados de libertad (es decir, que incluye 20 muestras o variables)&quot;}</span>
<span class="n"># generate the data </span>
<span class="n">n &lt;- 1000</span>
<span class="n">normal.a &lt;- rnorm( n )</span>
<span class="n">normal.b &lt;- rnorm( n )</span>
<span class="n">normal.c &lt;- rnorm( n )</span>
<span class="n">chi.sq.3 &lt;- (normal.a)^2 + (normal.b)^2 + (normal.c)^2	</span>
<span class="n">normal.d &lt;- rnorm( n )</span>
<span class="n">t.3 &lt;- normal.d / sqrt( chi.sq.3 / 3 )</span>
<span class="n">chi.sq.20 &lt;- rchisq( n, 20)</span>
<span class="n">F.3.20 &lt;- (chi.sq.3 / 3) / (chi.sq.20 / 20)</span>
<span class="n"># histogram for the F dist data</span>
<span class="n">bw &lt;- .2</span>
<span class="n">hist( 	F.3.20, seq(0,max(F.3.20)+bw,bw),</span>
<span class="n">       freq=FALSE, xlim=c(0,6), </span>
<span class="n">       col=ifelse(colour,emphColLight,emphGrey),</span>
<span class="n">       border=&quot;white&quot;, axes=FALSE, ylim=c(0,.7),</span>
<span class="n">       xlab=&quot;&quot;,ylab=&quot;&quot;, main=&quot;Distribución F&quot;,</span>
<span class="n">       font.main=1</span>
<span class="n">)</span>
<span class="n">lines( x&lt;-seq(0,6,.01), df(x,3,20), lwd=3, col=&quot;black&quot;  )	</span>
<span class="n">axis(1)</span>
<span class="n">```</span> 
</pre></div>
</div>
</div>
</div>
<p>Hemos visto tres nuevas distribuciones: <span class="math notranslate nohighlight">\(\chi^2\)</span>, <span class="math notranslate nohighlight">\(t\)</span> y <span class="math notranslate nohighlight">\(F\)</span>. Todas son distribuciones continuas, y todas están estrechamente relacionadas con la distribución normal. Hemos hablado un poco poco sobre la naturaleza de esta relación. Sin embargo, la clave no es que tengas una comprensión profunda y detallada de todas estas diferentes distribuciones, ni que recuerdes las relaciones precisas que existen entre ellas. Lo más importante es entender la idea básica de que estas distribuciones están profundamente relacionadas entre sí y a su vez con la distribución normal. Más adelante en el curso nos vamos a encontrar con datos que se distribuyen normalmente, o que al menos suponemos que se distribuyen normalmente. Por lo tanto, si suponemos que nuestros datos se distribuyen normalmente, debemos saber reconocer las distribuciones <span class="math notranslate nohighlight">\(\chi^2\)</span>, <span class="math notranslate nohighlight">\(t\)</span> y <span class="math notranslate nohighlight">\(F\)</span>..</p>
</section>
<section id="resumen">
<h2>Resumen<a class="headerlink" href="#resumen" title="Permalink to this heading">#</a></h2>
<p>En este capítulo hemos hablado de probabilidad. Hemos hablado de lo que significa la probabilidad y por qué los estadísticos no están muy de acuerdo en lo que significa. Hablamos sobre las reglas que las probabilidades tienen que obedecer. Hemos introducido la idea de una distribución de probabilidad y conocido algunas de las distribuciones de probabilidad más importantes con las que nos podemos encontrar. Los temas han sido los siguientes:</p>
<ul class="simple">
<li><p>Teoría de probabilidad vs estadística (Sección &#64;ref(probstats))</p></li>
<li><p>Visión frecuenciantista vs visión bayesiana de probabilidad (Sección &#64;ref(probmeaning))</p></li>
<li><p>Conceptos básicos de la teoría de probabilidad (Sección &#64;ref(basicprobability))</p></li>
<li><p>Distribución binomial (Sección &#64;ref(binomial)), distribución normal (sección &#64;ref(normal)), y otras distribuciones (Sección &#64;ref(otherdists))</p></li>
</ul>
<p>Esto es una simple introducción dentro de un gran tema. La teoría de la probabilidad es una rama de las matemáticas, completamente separada de su aplicación a la estadística y al análisis de datos. Como tales, hay miles de libros escritos sobre el tema y las universidades generalmente ofrecen clases dedicadas por completo a la teoría de la probabilidad. En este capítulo se han descrito cinco distribuciones de probabilidad estándar, pero existen <em>muchas</em> más que esas. Afortunadamente, estas distribuciones bastarán por el momento.</p>
<p>Los conceptos básicos que hemos adquirido en este capítulo servirán como fundamento para los siguientes dos. Existen muchas reglas sobre lo que se nos “permite” decir cuando hacemos inferencia estadística, y muchas de ellas pueden parecer arbitrarias. Sin embargo, veremos que comienzan a tener sentido si tenemos en cuenta estos conceptos básicos que hemos aprendido.</p>
</section>
</section>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "negatoscope/fundamentos_book",
            ref: "main",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "r"
        },
        kernelOptions: {
            name: "ir",
            path: "./docs"
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'ir'</script>

                </article>
              

              
              
                <footer class="bd-footer-article">
                  
<div class="footer-article-items footer-article__inner">
  
    <div class="footer-article-item"><!-- Previous / next buttons -->
<div class="prev-next-area">
</div></div>
  
</div>

                </footer>
              
            </div>
            
            
              
                <div class="bd-sidebar-secondary bd-toc"><div class="sidebar-secondary-items sidebar-secondary__inner">

  <div class="sidebar-secondary-item">
  <div class="page-toc tocsection onthispage">
    <i class="fa-solid fa-list"></i> Contents
  </div>
  <nav class="bd-toc-nav page-toc">
    <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#como-de-diferentes-son-la-probabilidad-y-la-estadistica-probstats">¿Cómo de diferentes son la probabilidad y la estadística?{#probstats}</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#que-significa-la-probabilidad-probmeaning">¿Qué significa la probabilidad?{#probmeaning}</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#la-vision-frecuentista">La visión frecuentista</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#la-vision-bayesiana">La visión bayesiana</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#cual-es-la-diferencia-y-quien-tiene-razon">¿Cuál es la diferencia? ¿Y quién tiene razón?</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#teoria-de-probabilidad-basica-basicprobability">Teoría de probabilidad básica{#basicprobability}</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#introduccion-a-las-distribuciones-de-probabilidad">Introducción a las distribuciones de probabilidad</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#la-distribucion-binomial-binomial">La distribución binomial{#binomial}</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#introduccion-al-binomio">Introducción al binomio</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#la-distribucion-normal-normal">La distribución normal{#normal}</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#densidad-de-probabilidad-density">Densidad de probabilidad{#density}</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#otras-distribuciones-utiles-otherdists">Otras distribuciones útiles{#otherdists}</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#resumen">Resumen</a></li>
</ul>
  </nav></div>

</div></div>
              
            
          </div>
          <footer class="bd-footer-content">
            
<div class="bd-footer-content__inner container">
  
  <div class="footer-item">
    
<p class="component-author">
By Luis Eudave
</p>

  </div>
  
  <div class="footer-item">
    
  <p class="copyright">
    
      © Copyright 2022.
      <br/>
    
  </p>

  </div>
  
  <div class="footer-item">
    
  </div>
  
  <div class="footer-item">
    
  </div>
  
</div>
          </footer>
        

      </main>
    </div>
  </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script src="../_static/scripts/bootstrap.js?digest=e353d410970836974a52"></script>
<script src="../_static/scripts/pydata-sphinx-theme.js?digest=e353d410970836974a52"></script>

  <footer class="bd-footer">
  </footer>
  </body>
</html>